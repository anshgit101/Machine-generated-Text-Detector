{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "8wNB8EqzRr6L"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQFO-3Hr2c4Y",
        "outputId": "a6c3cbe7-9166-4f9b-fb09-1058d68aa5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94OpNyuGEK46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcac7c9b-b223-443e-84ab-f55fb8d158f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create train test and validation\n",
        "\n",
        "trainAmt = 0.7\n",
        "validationTestRatio = 0.5\n",
        "\n",
        "with open('/content/drive/MyDrive/Datasets/Autextification_datasets/subtask_1/en/train.tsv', 'r') as f:\n",
        "  allData = f.readlines()\n",
        "\n",
        "np.random.shuffle(allData)\n",
        "\n",
        "trainIdx = int(np.floor(len(allData) * trainAmt))\n",
        "valIdx = int(np.floor(len(allData) * (1 - trainAmt) * validationTestRatio) + trainIdx)"
      ],
      "metadata": {
        "id": "3WV3TvidFBPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "labels = []\n",
        "\n",
        "for datum in tqdm(allData):\n",
        "  _, s, lbl = datum[:-1].split('\\t')\n",
        "  text.append(s)\n",
        "  labels.append(1 if lbl == 'generated' else 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKWpsloczsMn",
        "outputId": "d0ecde0d-82ad-45ea-d36f-f477542979f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33846/33846 [00:00<00:00, 757639.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 90,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    do_lower_case = True\n",
        "    )\n",
        "\n",
        "for sample in tqdm(text):\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uttb92FzhnI",
        "outputId": "1c6ba1d7-d3c9-4f91-ac89-879d5d4a0ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/33846 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "100%|██████████| 33846/33846 [01:06<00:00, 506.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into train val and test\n",
        "\n",
        "valntest_ratio = 0.3\n",
        "valtest_split = 0.5\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, temp_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = valntest_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = labels)\n",
        "\n",
        "val_idx, test_idx = train_test_split(\n",
        "    np.arange(len(labels[temp_idx])),\n",
        "    test_size = valtest_split,\n",
        "    shuffle = True,\n",
        "    stratify = labels[temp_idx])\n",
        "\n",
        "# Train and validation sets\n",
        "train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        labels[val_idx])\n",
        "\n",
        "test_set = TensorDataset(token_id[test_idx], \n",
        "                        attention_masks[test_idx], \n",
        "                        labels[test_idx])\n",
        "\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            test_set,\n",
        "            sampler = SequentialSampler(test_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "metadata": {
        "id": "nWE5tNEF1GoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "seq_len = [len(i.split()) for i in text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "vdDW8gM3xj00",
        "outputId": "ceef5be9-49e3-438d-ef83-5ce43e48c38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def b_tp(preds, labels):\n",
        "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fp(preds, labels):\n",
        "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_tn(preds, labels):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fn(preds, labels):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - specificity = TN / (TN + FP)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, labels)\n",
        "  tn = b_tn(preds, labels)\n",
        "  fp = b_fp(preds, labels)\n",
        "  fn = b_fn(preds, labels)\n",
        "  b_accuracy = (tp + tn) / len(labels)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
        "  return b_accuracy, b_precision, b_recall, b_specificity"
      ],
      "metadata": {
        "id": "HUtPn983S038"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 5e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "model.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxCyJf0L41ad",
        "outputId": "e3f4fa8e-eb00-40fd-9f65-a66eaedd6434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train naive single BERT"
      ],
      "metadata": {
        "id": "LAErG9Uz2OD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "print(\"Set length: \", len(train_dataloader))\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    updator = 100\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if (step+1) % updator == 0:\n",
        "          print(\"Step: \", step)\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        # print(\"Logits: \", train_output.logits)\n",
        "        # print(\"Labels: \", b_labels)\n",
        "\n",
        "        cntLoss = lossFn(train_output.logits, b_labels)\n",
        "\n",
        "        cntLoss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += cntLoss\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        # print(\"Loss: \", train_output.loss)\n",
        "        # print(\"My loss\", cntLoss)\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
      ],
      "metadata": {
        "id": "IrmupI6J4utl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b11285-5b5b-43d9-a26b-c9e7d6e44d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set length:  1481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n",
            "Step:  699\n",
            "Step:  799\n",
            "Step:  899\n",
            "Step:  999\n",
            "Step:  1099\n",
            "Step:  1199\n",
            "Step:  1299\n",
            "Step:  1399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 1/2 [07:00<07:00, 420.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3280\n",
            "\t - Validation Accuracy: 0.8316\n",
            "\t - Validation Precision: 0.7472\n",
            "\t - Validation Recall: 0.9946\n",
            "\t - Validation Specificity: 0.6796\n",
            "\n",
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n",
            "Step:  699\n",
            "Step:  799\n",
            "Step:  899\n",
            "Step:  999\n",
            "Step:  1099\n",
            "Step:  1199\n",
            "Step:  1299\n",
            "Step:  1399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [13:58<00:00, 419.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.1459\n",
            "\t - Validation Accuracy: 0.9625\n",
            "\t - Validation Precision: 0.9552\n",
            "\t - Validation Recall: 0.9710\n",
            "\t - Validation Specificity: 0.9558\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tune ensemble BERTs on subtask B\n",
        "- 6 BERTs, one for each label\n",
        "- Each one performs binary classification\n"
      ],
      "metadata": {
        "id": "1jP78BlS2T9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess subtask B data\n",
        "\n",
        "trainAmt = 0.7\n",
        "validationTestRatio = 0.5\n",
        "\n",
        "with open('/content/drive/MyDrive/Datasets/Autextification_datasets/subtask_2/en/train.tsv', 'r') as f:\n",
        "  allData = f.readlines()\n",
        "allData = allData[1:]\n",
        "\n",
        "np.random.shuffle(allData)\n",
        "\n",
        "trainIdx = int(np.floor(len(allData) * trainAmt))\n",
        "valIdx = int(np.floor(len(allData) * (1 - trainAmt) * validationTestRatio) + trainIdx)\n",
        "\n",
        "text = []\n",
        "labels = []\n",
        "\n",
        "letter2lbl = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5}\n",
        "\n",
        "for datum in tqdm(allData):\n",
        "  _, s, letter = datum[:-1].split('\\t')\n",
        "  text.append(s)\n",
        "  labels.append(letter2lbl[letter])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUjG-WmN2bb0",
        "outputId": "4dc10bd7-78dd-4174-fdec-66856a3e089b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22416/22416 [00:00<00:00, 301564.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess data\n",
        "\n",
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 90,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    do_lower_case = True\n",
        "    )\n",
        "\n",
        "for sample in tqdm(text):\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzx1E7btH5rr",
        "outputId": "71689c70-a664-4240-b847-33c4f3eda7e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/22416 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "100%|██████████| 22416/22416 [00:36<00:00, 621.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders\n",
        "# split into train val and test\n",
        "\n",
        "numModels = 6 # ONE FOR EACH CLASS\n",
        "\n",
        "valntest_ratio = 0.3\n",
        "valtest_split = 0.5\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, temp_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = valntest_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = labels)\n",
        "\n",
        "val_idx, test_idx = train_test_split(\n",
        "    np.arange(len(labels[temp_idx])),\n",
        "    test_size = valtest_split,\n",
        "    shuffle = True,\n",
        "    stratify = labels[temp_idx])\n",
        "\n",
        "\n",
        "# Train and validation sets\n",
        "train_sets = []\n",
        "val_sets = []\n",
        "test_sets = []\n",
        "\n",
        "train_dataloaders = []\n",
        "validation_dataloaders = []\n",
        "test_dataloaders = []\n",
        "\n",
        "a = 100\n",
        "b = 110\n",
        "\n",
        "print(\"OG   \", end='\\t')\n",
        "print(labels[a:b])\n",
        "\n",
        "# 6 class datasets\n",
        "six_class_train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          labels[train_idx])\n",
        "\n",
        "six_class_val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        labels[val_idx])\n",
        "\n",
        "six_class_test_set = TensorDataset(token_id[test_idx], \n",
        "                        attention_masks[test_idx], \n",
        "                        labels[test_idx])\n",
        "\n",
        "six_class_train_dataloader = DataLoader(\n",
        "              six_class_train_set,\n",
        "              sampler = RandomSampler(six_class_train_set),\n",
        "              batch_size = batch_size\n",
        "          )\n",
        "\n",
        "six_class_validation_dataloader = DataLoader(\n",
        "            six_class_val_set,\n",
        "            sampler = SequentialSampler(six_class_val_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "six_class_test_dataloader = DataLoader(\n",
        "            six_class_test_set,\n",
        "            sampler = SequentialSampler(six_class_test_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "\n",
        "# binary classification datasets\n",
        "for i in range(numModels):\n",
        "  # label reassignment: i -> 1, not i -> 0\n",
        "  reassignedLabels = torch.tensor(np.where(np.array(labels) == i, 1, 0))\n",
        "\n",
        "  print(\"LABEL %d\" % i, end='\\t')\n",
        "  print(reassignedLabels[a:b])\n",
        "  \n",
        "  train_sets.append(TensorDataset(token_id[train_idx], \n",
        "                            attention_masks[train_idx], \n",
        "                            reassignedLabels[train_idx]))\n",
        "\n",
        "  val_sets.append(TensorDataset(token_id[val_idx], \n",
        "                          attention_masks[val_idx], \n",
        "                          reassignedLabels[val_idx]))\n",
        "\n",
        "  test_sets.append(TensorDataset(token_id[test_idx], \n",
        "                          attention_masks[test_idx], \n",
        "                          reassignedLabels[test_idx]))\n",
        "\n",
        "\n",
        "  # Prepare DataLoader\n",
        "  train_dataloaders.append(DataLoader(\n",
        "              train_sets[i],\n",
        "              sampler = RandomSampler(train_sets[i]),\n",
        "              batch_size = batch_size\n",
        "          ))\n",
        "\n",
        "  validation_dataloaders.append(DataLoader(\n",
        "              val_sets[i],\n",
        "              sampler = SequentialSampler(val_sets[i]),\n",
        "              batch_size = batch_size\n",
        "          ))\n",
        "\n",
        "  test_dataloaders.append(DataLoader(\n",
        "              test_sets[i],\n",
        "              sampler = SequentialSampler(test_sets[i]),\n",
        "              batch_size = batch_size\n",
        "          ))\n",
        "  \n",
        "  print(\"counts: \", end='\\t')\n",
        "  counts = [0,0,0,0,0,0]\n",
        "  for el in tqdm(reassignedLabels):\n",
        "    counts[el] += 1\n",
        "  print(counts)\n",
        "\n",
        "  print(\"lens: \", len(train_dataloaders[i]), len(validation_dataloaders[i]), len(test_dataloaders[i]))\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "xOQVJmD5J6A3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bce06ac-12c6-4250-c976-0b9964f2f929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OG   \ttensor([0, 3, 5, 4, 3, 4, 0, 5, 2, 3])\n",
            "LABEL 0\ttensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
            "counts: \t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22416/22416 [00:00<00:00, 102777.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18854, 3562, 0, 0, 0, 0]\n",
            "lens:  981 211 211\n",
            "\n",
            "LABEL 1\ttensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "counts: \t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22416/22416 [00:00<00:00, 97604.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18768, 3648, 0, 0, 0, 0]\n",
            "lens:  981 211 211\n",
            "\n",
            "LABEL 2\ttensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
            "counts: \t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22416/22416 [00:00<00:00, 33317.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18729, 3687, 0, 0, 0, 0]\n",
            "lens:  981 211 211\n",
            "\n",
            "LABEL 3\ttensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1])\n",
            "counts: \t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22416/22416 [00:00<00:00, 68862.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18546, 3870, 0, 0, 0, 0]\n",
            "lens:  981 211 211\n",
            "\n",
            "LABEL 4\ttensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
            "counts: \t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22416/22416 [00:00<00:00, 197195.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18594, 3822, 0, 0, 0, 0]\n",
            "lens:  981 211 211\n",
            "\n",
            "LABEL 5\ttensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n",
            "counts: \t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22416/22416 [00:00<00:00, 63354.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18589, 3827, 0, 0, 0, 0]\n",
            "lens:  981 211 211\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation functions\n",
        "\n",
        "def b_tp(preds, labels):\n",
        "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fp(preds, labels):\n",
        "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_tn(preds, labels):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fn(preds, labels):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - specificity = TN / (TN + FP)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, labels)\n",
        "  tn = b_tn(preds, labels)\n",
        "  fp = b_fp(preds, labels)\n",
        "  fn = b_fn(preds, labels)\n",
        "  b_accuracy = (tp + tn) / len(labels)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
        "  return b_accuracy, b_precision, b_recall, b_specificity"
      ],
      "metadata": {
        "id": "D708PbeMPXme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 class BERT\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "six_bert = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    num_labels = 6,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "  ).to(device)\n",
        "\n",
        "model = six_bert\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                            lr = 5e-5,\n",
        "                            eps = 1e-08\n",
        "                            )\n",
        "lossFn = nn.CrossEntropyLoss().cuda()\n",
        "epochs = 2\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    updator=100\n",
        "    for step, batch in enumerate(six_class_train_dataloader):\n",
        "        if (step + 1) % 100 == 0:\n",
        "          print(\"Step: \", step)\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                            token_type_ids = None, \n",
        "                            attention_mask = b_input_mask, \n",
        "                            labels = b_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        cntLoss = lossFn(train_output.logits, b_labels)\n",
        "        cntLoss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += cntLoss\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "\n",
        "    all_labels = np.array([])\n",
        "    all_preds = np.array([])\n",
        "\n",
        "    for batch in six_class_validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        all_labels = np.append(all_labels, label_ids)\n",
        "        all_preds = np.append(all_preds, np.argmax(logits, axis = 1).flatten())\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
        "\n",
        "    from sklearn.metrics import classification_report   \n",
        "    print(classification_report(all_labels, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4hreCeOVuZX",
        "outputId": "bb46e322-0ee5-4182-be29-af76c0b6c089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n",
            "Step:  699\n",
            "Step:  799\n",
            "Step:  899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 1/2 [05:00<05:00, 300.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 1.2704\n",
            "\t - Validation Accuracy: 0.2085\n",
            "\t - Validation Precision: 0.4528\n",
            "\t - Validation Recall: 0.7583\n",
            "\t - Validation Specificity: 0.4603\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.64      0.71       540\n",
            "         1.0       0.46      0.65      0.54       541\n",
            "         2.0       0.62      0.40      0.48       573\n",
            "         3.0       0.43      0.65      0.52       569\n",
            "         4.0       0.43      0.34      0.38       537\n",
            "         5.0       0.88      0.73      0.80       602\n",
            "\n",
            "    accuracy                           0.57      3362\n",
            "   macro avg       0.60      0.57      0.57      3362\n",
            "weighted avg       0.60      0.57      0.57      3362\n",
            "\n",
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n",
            "Step:  699\n",
            "Step:  799\n",
            "Step:  899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [10:00<00:00, 300.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.9491\n",
            "\t - Validation Accuracy: 0.1919\n",
            "\t - Validation Precision: 0.5562\n",
            "\t - Validation Recall: 0.8992\n",
            "\t - Validation Specificity: 0.6092\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.65      0.76       540\n",
            "         1.0       0.57      0.54      0.55       541\n",
            "         2.0       0.56      0.68      0.61       573\n",
            "         3.0       0.47      0.15      0.22       569\n",
            "         4.0       0.46      0.64      0.53       537\n",
            "         5.0       0.68      0.95      0.79       602\n",
            "\n",
            "    accuracy                           0.60      3362\n",
            "   macro avg       0.61      0.60      0.58      3362\n",
            "weighted avg       0.61      0.60      0.58      3362\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble BERT"
      ],
      "metadata": {
        "id": "8wNB8EqzRr6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define all models\n",
        "\n",
        "BERTs = []\n",
        "for i in range(numModels):\n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "  )\n",
        "  model.cuda()\n",
        "\n",
        "  BERTs.append(model)"
      ],
      "metadata": {
        "id": "JyBOaMArQOVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train each BERT\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "print(\"Trainset length: \", len(train_dataloader))\n",
        "for bertNum in range(numModels):\n",
        "  print(\"TRAINING BERT #%d\" % bertNum)\n",
        "\n",
        "  train_dataloader = train_dataloaders[bertNum]\n",
        "  validation_dataloader = validation_dataloaders[bertNum]\n",
        "\n",
        "\n",
        "  print(train_dataloaders[bertNum])\n",
        "\n",
        "  model = BERTs[bertNum]\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 5e-7,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "  classWeights = torch.tensor([1., 100.]).float().cuda()\n",
        "  print(classWeights)\n",
        "  lossFn = nn.CrossEntropyLoss(weight=classWeights)\n",
        "\n",
        "  for _ in trange(epochs, desc = 'Epoch'):\n",
        "      \n",
        "      # ========== Training ==========\n",
        "      \n",
        "      # Set model to training mode\n",
        "      model.train()\n",
        "      \n",
        "      # Tracking variables\n",
        "      tr_loss = 0\n",
        "      nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "      updator=100\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "          if (step + 1) % 100 == 0:\n",
        "            print(\"Step: \", step)\n",
        "\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          optimizer.zero_grad()\n",
        "          # Forward pass\n",
        "          train_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask, \n",
        "                              labels = b_labels)\n",
        "          # Backward pass\n",
        "          cntLoss = lossFn(train_output.logits, b_labels)\n",
        "          cntLoss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "          # Update tracking variables\n",
        "          tr_loss += cntLoss\n",
        "          nb_tr_examples += b_input_ids.size(0)\n",
        "          nb_tr_steps += 1\n",
        "\n",
        "      # ========== Validation ==========\n",
        "\n",
        "      # Set model to evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      val_accuracy = []\n",
        "      val_precision = []\n",
        "      val_recall = []\n",
        "      val_specificity = []\n",
        "\n",
        "      for batch in validation_dataloader:\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          with torch.no_grad():\n",
        "            # Forward pass\n",
        "            eval_output = model(b_input_ids, \n",
        "                                token_type_ids = None, \n",
        "                                attention_mask = b_input_mask)\n",
        "          logits = eval_output.logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "          # Calculate validation metrics\n",
        "          b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "          val_accuracy.append(b_accuracy)\n",
        "          # Update precision only when (tp + fp) !=0; ignore nan\n",
        "          if b_precision != 'nan': val_precision.append(b_precision)\n",
        "          # Update recall only when (tp + fn) !=0; ignore nan\n",
        "          if b_recall != 'nan': val_recall.append(b_recall)\n",
        "          # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "          if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "      print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "      print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "      print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "      print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "      print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "AuY1AL-XQpAm",
        "outputId": "74538cf0-c1c9-4006-fa28-2b6052096d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainset length:  981\n",
            "TRAINING BERT #0\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f217e3652a0>\n",
            "tensor([  1., 100.], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n",
            "Step:  699\n",
            "Step:  799\n",
            "Step:  899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 1/2 [04:34<04:34, 274.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 2.0005\n",
            "\t - Validation Accuracy: 0.8320\n",
            "\t - Validation Precision: NaN\n",
            "\t - Validation Recall: 0.0000\n",
            "\t - Validation Specificity: 1.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 1/2 [04:47<04:47, 287.03s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-a0c935d416d8>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m           \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m           \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-a0c935d416d8>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m           \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m           \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical training\n",
        "\n",
        "BERT seems to learn how to classify classes 0 and 5 very well, but cant disstinguish between the rest.\n",
        "\n",
        "We will now try a hierarchical approach, first discriminating between (0,5 and not-0-or-5), and then training a separate model to further classify the not-0-or-5 entries into 1,2,3 and 4 "
      ],
      "metadata": {
        "id": "G2HfA4cOlN5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "tree so far:\n",
        "\n",
        "    ___\n",
        "   / | \\\n",
        "  0  5  else\n",
        "        -+-\n",
        "      / | | \\\n",
        "     1  2 3  4\n",
        "\n",
        "FIRST BRANCH:\n",
        "remapping \n",
        "1,2,3,4   -> 0\n",
        "0         -> 1\n",
        "5         -> 2\n",
        "\n",
        "\n",
        "SECOND BRANCH:\n",
        "remapping\n",
        "1         -> 0\n",
        "2         -> 1\n",
        "3         -> 2\n",
        "4         -> 3\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# convert labels to first branch mappings\n",
        "first_node_labels = torch.tensor(np.zeros_like(labels))\n",
        "first_node_labels[(labels == 0).nonzero()] = 1\n",
        "first_node_labels[(labels == 5).nonzero()] = 2\n",
        "\n",
        "\n",
        "first_node_train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          first_node_labels[train_idx])\n",
        "\n",
        "first_node_val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        first_node_labels[val_idx])\n",
        "\n",
        "first_node_test_set = TensorDataset(token_id[test_idx], \n",
        "                        attention_masks[test_idx], \n",
        "                        first_node_labels[test_idx])\n",
        "\n",
        "first_node_train_dataloader = DataLoader(\n",
        "            first_node_train_set,\n",
        "            sampler = RandomSampler(first_node_train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "first_node_validation_dataloader = DataLoader(\n",
        "            first_node_val_set,\n",
        "            sampler = SequentialSampler(first_node_val_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "first_node_test_dataloader = DataLoader(\n",
        "            first_node_test_set,\n",
        "            sampler = SequentialSampler(first_node_test_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "  \n",
        "print(\"first branch counts: \", end='\\t')\n",
        "counts = [0,0,0,0,0,0]\n",
        "for el in tqdm(first_node_labels):\n",
        "  counts[el] += 1\n",
        "print(counts)\n",
        "\n",
        "print(\"lens: \", len(first_node_train_dataloader), len(first_node_validation_dataloader), len(first_node_test_dataloader))\n",
        "print()\n",
        "\n",
        "\n",
        "# get only second node branch mappings\n",
        "# second_node_labels = labels[labels != 0]\n",
        "# second_node_labels = second_node_labels[second_node_labels != 5]\n",
        "\n",
        "# temp = np.copy(second_node_labels)\n",
        "# second_node_labels[(temp == 1).nonzero()] = 0\n",
        "# second_node_labels[(temp == 2).nonzero()] = 1\n",
        "# second_node_labels[(temp == 3).nonzero()] = 2\n",
        "# second_node_labels[(temp == 4).nonzero()] = 3\n",
        "\n",
        "second_node_labels = torch.tensor(np.copy(labels))\n",
        "second_node_labels[(labels == 0).nonzero()] = 9\n",
        "second_node_labels[(labels == 1).nonzero()] = 0\n",
        "second_node_labels[(labels == 2).nonzero()] = 1\n",
        "second_node_labels[(labels == 3).nonzero()] = 2\n",
        "second_node_labels[(labels == 4).nonzero()] = 3\n",
        "second_node_labels[(labels == 5).nonzero()] = 9\n",
        "\n",
        "second_node_labels = second_node_labels[second_node_labels != 9]\n",
        "\n",
        "# get second branch indices\n",
        "second_train_idx, second_temp_idx = train_test_split(\n",
        "    np.arange(len(second_node_labels)),\n",
        "    test_size = valntest_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = second_node_labels)\n",
        "\n",
        "second_val_idx, second_test_idx = train_test_split(\n",
        "    np.arange(len(second_node_labels[second_temp_idx])),\n",
        "    test_size = valtest_split,\n",
        "    shuffle = True,\n",
        "    stratify = second_node_labels[second_temp_idx])\n",
        "\n",
        "\n",
        "second_node_train_set = TensorDataset(token_id[second_train_idx], \n",
        "                          attention_masks[second_train_idx], \n",
        "                          second_node_labels[second_train_idx])\n",
        "\n",
        "second_node_val_set = TensorDataset(token_id[second_val_idx], \n",
        "                        attention_masks[second_val_idx], \n",
        "                        second_node_labels[second_val_idx])\n",
        "\n",
        "second_node_test_set = TensorDataset(token_id[second_test_idx], \n",
        "                        attention_masks[second_test_idx], \n",
        "                        second_node_labels[second_test_idx])\n",
        "\n",
        "second_node_train_dataloader = DataLoader(\n",
        "            second_node_train_set,\n",
        "            sampler = RandomSampler(second_node_train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "second_node_validation_dataloader = DataLoader(\n",
        "            second_node_val_set,\n",
        "            sampler = SequentialSampler(second_node_val_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "second_node_test_dataloader = DataLoader(\n",
        "            second_node_test_set,\n",
        "            sampler = SequentialSampler(second_node_test_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "print(labels)\n",
        "print(first_node_labels)\n",
        "print(second_node_labels)\n",
        "\n",
        "for i in range(0,50):\n",
        "  print(int(labels[i]), end=' ')\n",
        "\n",
        "print()\n",
        "\n",
        "for i in range(0,50):\n",
        "  print(int(second_node_labels[i]), end=' ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXKWuaPDlyfI",
        "outputId": "07eca8e2-af7e-4fd7-eb24-5e7979f10372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first branch counts: \t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22416/22416 [00:00<00:00, 423202.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15027, 3562, 3827, 0, 0, 0]\n",
            "lens:  981 211 211\n",
            "\n",
            "tensor([3, 0, 4,  ..., 0, 4, 5])\n",
            "tensor([0, 1, 0,  ..., 1, 0, 2])\n",
            "tensor([2, 3, 3,  ..., 1, 2, 3])\n",
            "3 0 4 4 3 5 2 4 1 4 0 1 4 1 2 5 5 3 1 3 4 5 5 1 2 4 5 0 4 5 4 0 0 5 4 4 2 2 4 4 4 2 5 4 3 2 0 4 2 2 \n",
            "2 3 3 2 1 3 0 3 0 3 0 1 2 0 2 3 0 1 3 3 3 3 3 1 1 3 3 3 1 3 2 1 3 1 1 0 0 2 2 3 0 2 3 2 0 1 3 3 2 2 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = [0,0,0,0]\n",
        "for k in second_node_train_dataloader:\n",
        "  for e in k[-1]:\n",
        "    counts[e] += 1\n",
        "\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE-iNSIJzqrM",
        "outputId": "c1ee8e2c-bf92-4a63-8da2-8953f8687442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2553, 2581, 2709, 2675]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train both BERTs on each dataset separately\n",
        "# during inference, things that first_node_bert classifies as '0' are sent to second_node_bert for further classification\n",
        "# training occurs separately\n",
        "\n",
        "# train first bert\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "first_node_bert = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "  ).to(device)\n",
        "\n",
        "model = first_node_bert\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                            lr = 5e-5,\n",
        "                            eps = 1e-08\n",
        "                            )\n",
        "lossFn = nn.CrossEntropyLoss().cuda()\n",
        "epochs = 2\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    updator=100\n",
        "    for step, batch in enumerate(first_node_train_dataloader):\n",
        "        if (step + 1) % 100 == 0:\n",
        "          print(\"Step: \", step)\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                            token_type_ids = None, \n",
        "                            attention_mask = b_input_mask, \n",
        "                            labels = b_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        cntLoss = lossFn(train_output.logits, b_labels)\n",
        "        cntLoss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += cntLoss\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "\n",
        "    all_labels = np.array([])\n",
        "    all_preds = np.array([])\n",
        "\n",
        "    for batch in first_node_validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        all_labels = np.append(all_labels, label_ids)\n",
        "        all_preds = np.append(all_preds, np.argmax(logits, axis = 1).flatten())\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
        "\n",
        "    from sklearn.metrics import classification_report   \n",
        "    print(classification_report(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJZwPzdPqmzb",
        "outputId": "5dfdc73d-a3b5-49a7-8c33-c945c40cdead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n",
            "Step:  699\n",
            "Step:  799\n",
            "Step:  899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 1/2 [04:46<04:46, 286.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.5838\n",
            "\t - Validation Accuracy: 0.7068\n",
            "\t - Validation Precision: 0.7633\n",
            "\t - Validation Recall: 0.5136\n",
            "\t - Validation Specificity: 0.9480\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.92      0.89      2220\n",
            "         1.0       0.76      0.64      0.69       540\n",
            "         2.0       0.85      0.76      0.80       602\n",
            "\n",
            "    accuracy                           0.84      3362\n",
            "   macro avg       0.82      0.77      0.79      3362\n",
            "weighted avg       0.84      0.84      0.84      3362\n",
            "\n",
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n",
            "Step:  699\n",
            "Step:  799\n",
            "Step:  899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [09:46<00:00, 293.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3642\n",
            "\t - Validation Accuracy: 0.7453\n",
            "\t - Validation Precision: 0.9334\n",
            "\t - Validation Recall: 0.5089\n",
            "\t - Validation Specificity: 0.9898\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.98      0.92      2220\n",
            "         1.0       0.94      0.62      0.74       540\n",
            "         2.0       0.94      0.80      0.87       602\n",
            "\n",
            "    accuracy                           0.89      3362\n",
            "   macro avg       0.92      0.80      0.84      3362\n",
            "weighted avg       0.89      0.89      0.88      3362\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train second bert\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "second_node_bert = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    num_labels = 4,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "  ).to(device)\n",
        "\n",
        "model = second_node_bert\n",
        "lossFn = nn.CrossEntropyLoss().cuda()\n",
        "epochs = 4\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    # ========== Training ==========\n",
        "    if epochs == 0 or epochs == 1:\n",
        "      optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                                  lr = 1e-5,\n",
        "                                  eps = 1e-08\n",
        "                                  )\n",
        "    elif epochs == 2:\n",
        "      optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                                  lr = 7e-6,\n",
        "                                  eps = 1e-08\n",
        "                                  )\n",
        "    \n",
        "    elif epochs == 3:\n",
        "      optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                                  lr = 3e-6,\n",
        "                                  eps = 1e-08\n",
        "                                  )\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    updator=100\n",
        "    for step, batch in enumerate(second_node_train_dataloader):\n",
        "        if (step + 1) % 100 == 0:\n",
        "          print(\"Step: \", step)\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                            token_type_ids = None, \n",
        "                            attention_mask = b_input_mask, \n",
        "                            labels = b_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        cntLoss = lossFn(train_output.logits, b_labels)\n",
        "        cntLoss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += cntLoss\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "\n",
        "    all_labels = np.array([])\n",
        "    all_preds = np.array([])\n",
        "\n",
        "    for batch in second_node_validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        all_labels = np.append(all_labels, label_ids)\n",
        "        all_preds = np.append(all_preds, np.argmax(logits, axis = 1).flatten())\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
        "\n",
        "    from sklearn.metrics import classification_report   \n",
        "    print(classification_report(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yqnjtQ2vqJU",
        "outputId": "a015f831-9c7e-491b-8ee4-ba60d449cbab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\rEpoch:  25%|██▌       | 1/4 [02:46<08:20, 166.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 1.4412\n",
            "\t - Validation Accuracy: 0.2053\n",
            "\t - Validation Precision: 0.2181\n",
            "\t - Validation Recall: 0.0272\n",
            "\t - Validation Specificity: 0.7640\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.23      0.81      0.36       524\n",
            "         1.0       0.24      0.08      0.12       514\n",
            "         2.0       0.00      0.00      0.00       624\n",
            "         3.0       0.27      0.12      0.17       592\n",
            "\n",
            "    accuracy                           0.24      2254\n",
            "   macro avg       0.18      0.25      0.16      2254\n",
            "weighted avg       0.18      0.24      0.15      2254\n",
            "\n",
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\rEpoch:  50%|█████     | 2/4 [05:32<05:32, 166.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 1.4383\n",
            "\t - Validation Accuracy: 0.2053\n",
            "\t - Validation Precision: 0.2181\n",
            "\t - Validation Recall: 0.0272\n",
            "\t - Validation Specificity: 0.7640\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.23      0.81      0.36       524\n",
            "         1.0       0.24      0.08      0.12       514\n",
            "         2.0       0.00      0.00      0.00       624\n",
            "         3.0       0.27      0.12      0.17       592\n",
            "\n",
            "    accuracy                           0.24      2254\n",
            "   macro avg       0.18      0.25      0.16      2254\n",
            "weighted avg       0.18      0.24      0.15      2254\n",
            "\n",
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\rEpoch:  75%|███████▌  | 3/4 [08:18<02:45, 165.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 1.4381\n",
            "\t - Validation Accuracy: 0.2053\n",
            "\t - Validation Precision: 0.2181\n",
            "\t - Validation Recall: 0.0272\n",
            "\t - Validation Specificity: 0.7640\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.23      0.81      0.36       524\n",
            "         1.0       0.24      0.08      0.12       514\n",
            "         2.0       0.00      0.00      0.00       624\n",
            "         3.0       0.27      0.12      0.17       592\n",
            "\n",
            "    accuracy                           0.24      2254\n",
            "   macro avg       0.18      0.25      0.16      2254\n",
            "weighted avg       0.18      0.24      0.15      2254\n",
            "\n",
            "Step:  99\n",
            "Step:  199\n",
            "Step:  299\n",
            "Step:  399\n",
            "Step:  499\n",
            "Step:  599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch: 100%|██████████| 4/4 [11:03<00:00, 165.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 1.4362\n",
            "\t - Validation Accuracy: 0.2053\n",
            "\t - Validation Precision: 0.2181\n",
            "\t - Validation Recall: 0.0272\n",
            "\t - Validation Specificity: 0.7640\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.23      0.81      0.36       524\n",
            "         1.0       0.24      0.08      0.12       514\n",
            "         2.0       0.00      0.00      0.00       624\n",
            "         3.0       0.27      0.12      0.17       592\n",
            "\n",
            "    accuracy                           0.24      2254\n",
            "   macro avg       0.18      0.25      0.16      2254\n",
            "weighted avg       0.18      0.24      0.15      2254\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}