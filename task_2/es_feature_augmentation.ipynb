{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ab5284-41b9-4d0f-b529-2fa7b29d9c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0cc42b-5960-44a4-b0a0-4a251e5e532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a39071-e43f-46a6-893f-ce219f6dd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240511a7-0cd8-4abc-9d00-bf68a4ba7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 25\n",
    "# random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724952d7-1d42-4f71-acfc-dbf71b58217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                               text label\n",
      "0  12786  Sin embargo, los jóvenes son capaces de recono...     B\n",
      "1  12361  ¿Hay algo más que quieras compartir? ¿Algo sob...     B\n",
      "2   1662  El servicio de sala es bueno, rápido y amabilí...     B\n",
      "3  14729  Para concentrarse en el hablante, trata de des...     F\n",
      "4   9312  Los responsables locales tendrán ahora que esp...     F\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('datasets/subtask_2/es/train.tsv',sep='\\t')\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "883429df-a252-41d8-9e6a-cf224bf9951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:  17766\n",
      "validation data size:  1975\n",
      "test data size:  2194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data_texts = train_data['text'].to_list()\n",
    "train_data_labels = train_data['label'].to_list()\n",
    "train_data_labels = [ord(label)-ord('A') for label in train_data_labels]\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_data_texts, train_data_labels, test_size=0.1, random_state=25)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=25)\n",
    "print('train data size: ', len(train_texts))\n",
    "print('validation data size: ', len(val_texts))\n",
    "print('test data size: ', len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246a21d5-d834-4052-8a29-7c1d4325e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configurations\n",
      "\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "bert_model = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\").to(device)\n",
    "print(\"Model Configurations\")\n",
    "print()\n",
    "print(bert_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05431d70-74a3-49dc-b3e5-7cbf6b007c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 17766/17766 [02:52<00:00, 103.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape:  (17766, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    # Tokenize input text\n",
    "    encoded_input = bert_tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "    #get bert embeddings\n",
    "    with torch.no_grad():\n",
    "        bert_output = bert_model(**encoded_input)\n",
    "    bert_embeddings = bert_output.last_hidden_state[:,0,:].cpu().numpy()\n",
    "    return bert_embeddings\n",
    "\n",
    "#get train embeddings\n",
    "train_embeddings = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_embeddings.append(get_bert_embeddings(text))\n",
    "train_embeddings = np.array(train_embeddings)\n",
    "train_embeddings = np.squeeze(train_embeddings, axis=1)\n",
    "print('train embeddings shape: ', train_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7050edb3-44bd-4884-8ac3-53bb6ea4c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1975/1975 [00:18<00:00, 106.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation embeddings shape:  (1975, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2194/2194 [00:20<00:00, 106.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test embeddings shape:  (2194, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#get validation embeddings\n",
    "val_embeddings = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_embeddings.append(get_bert_embeddings(text))\n",
    "val_embeddings = np.array(val_embeddings)\n",
    "val_embeddings = np.squeeze(val_embeddings, axis=1)\n",
    "print('validation embeddings shape: ', val_embeddings.shape) #shape: (num_samples, 1, 768)\n",
    "\n",
    "\n",
    "#get test embeddings\n",
    "test_embeddings = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_embeddings.append(get_bert_embeddings(text))\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "test_embeddings = np.squeeze(test_embeddings, axis=1)\n",
    "print('test embeddings shape: ', test_embeddings.shape) #shape: (num_samples, 1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9983d3f9-9d90-4452-aace-f8b6f436a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train punc shape:  (17766,)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def count_punctuations(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return count\n",
    "\n",
    "train_punc = []\n",
    "for text in train_texts:\n",
    "    train_punc.append(count_punctuations(text))\n",
    "train_punc = np.array(train_punc)\n",
    "\n",
    "val_punc = []\n",
    "for text in val_texts:\n",
    "    val_punc.append(count_punctuations(text))\n",
    "val_punc = np.array(val_punc)\n",
    "\n",
    "test_punc = []\n",
    "for text in test_texts:\n",
    "    test_punc.append(count_punctuations(text))\n",
    "test_punc = np.array(test_punc)\n",
    "print('train punc shape: ', train_punc.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06d55ee-e255-4047-b08e-e2cb194a37f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train capital shape:  (17766,)\n"
     ]
    }
   ],
   "source": [
    "def count_capital_letters(text):\n",
    "    count = sum([1 for char in text if char.isupper()])\n",
    "    return count\n",
    "\n",
    "train_capital = []\n",
    "for text in train_texts:\n",
    "    train_capital.append(count_capital_letters(text))\n",
    "train_capital = np.array(train_capital)\n",
    "\n",
    "val_capital = []\n",
    "for text in val_texts:\n",
    "    val_capital.append(count_capital_letters(text))\n",
    "val_capital = np.array(val_capital)\n",
    "\n",
    "test_capital = []\n",
    "for text in test_texts:\n",
    "    test_capital.append(count_capital_letters(text))\n",
    "test_capital = np.array(test_capital)\n",
    "print('train capital shape: ', train_capital.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab44401c-5ed6-4441-93f3-c9170c4edf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform sentiment analysis on a spanish text\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5380762c-118f-4560-a620-b78aaf0bc5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 17766/17766 [14:01<00:00, 21.12it/s]\n",
      "100%|███████████████████████████████████████| 1975/1975 [01:33<00:00, 21.09it/s]\n",
      "100%|███████████████████████████████████████| 2194/2194 [01:42<00:00, 21.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentiment shape:  (17766,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    sentiment = sentiment_analysis(text)[0]['label']\n",
    "    #remove stars\n",
    "    #if its 1 star return 1\n",
    "    if sentiment == '1 star':\n",
    "        sentiment = 1\n",
    "    #if its 2 stars return 2\n",
    "    elif sentiment == '2 stars':\n",
    "        sentiment = 2\n",
    "    #if its 3 stars return 3\n",
    "    elif sentiment == '3 stars':\n",
    "        sentiment = 3\n",
    "    #if its 4 stars return 4\n",
    "    elif sentiment == '4 stars':\n",
    "        sentiment = 4\n",
    "    #if its 5 stars return 5\n",
    "    elif sentiment == '5 stars':\n",
    "        sentiment = 5\n",
    "    return sentiment #dim: (num_samples, 1) range: [1,5]\n",
    "\n",
    "train_sentiment = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_sentiment.append(get_sentiment(text))\n",
    "train_sentiment = np.array(train_sentiment)\n",
    "\n",
    "val_sentiment = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_sentiment.append(get_sentiment(text))\n",
    "val_sentiment = np.array(val_sentiment)\n",
    "\n",
    "test_sentiment = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_sentiment.append(get_sentiment(text))\n",
    "test_sentiment = np.array(test_sentiment)\n",
    "print('train sentiment shape: ', train_sentiment.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a2ac049-cc43-4c2a-860d-3c09d742953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 17766/17766 [02:46<00:00, 106.97it/s]\n",
      "100%|██████████████████████████████████████| 1975/1975 [00:17<00:00, 109.91it/s]\n",
      "100%|██████████████████████████████████████| 2194/2194 [00:20<00:00, 108.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pos shape:  (17766, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "#function to get pos tags for each category\n",
    "def get_pos(text):\n",
    "    doc = nlp(text)\n",
    "    adj_count = 0\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adp_count = 0\n",
    "    det_count = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            adj_count += 1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun_count += 1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb_count += 1\n",
    "        elif token.pos_ == 'ADP':\n",
    "            adp_count += 1\n",
    "        elif token.pos_ == 'DET':\n",
    "            det_count += 1\n",
    "    return [adj_count, noun_count, verb_count, adp_count, det_count] #dim: (num_samples, 5) \n",
    "\n",
    "train_pos = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_pos.append(get_pos(text))\n",
    "train_pos = np.array(train_pos)\n",
    "\n",
    "val_pos = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_pos.append(get_pos(text))\n",
    "val_pos = np.array(val_pos)\n",
    "\n",
    "test_pos = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_pos.append(get_pos(text))\n",
    "test_pos = np.array(test_pos)\n",
    "print('train pos shape: ', train_pos.shape) #shape: (num_samples, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67b74ff8-322c-4800-a2b1-960d2c618740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "ner_analysis = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d1fe338-ffd1-438f-974e-853411dc875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/17766 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|█████████████████████████████████████| 17766/17766 [13:07<00:00, 22.56it/s]\n",
      "100%|███████████████████████████████████████| 1975/1975 [01:28<00:00, 22.40it/s]\n",
      "100%|███████████████████████████████████████| 2194/2194 [01:38<00:00, 22.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ner shape:  (17766, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner = ner_analysis(text)\n",
    "    loc_count = 0\n",
    "    org_count = 0\n",
    "    per_count = 0\n",
    "    misc_count = 0\n",
    "    for item in ner:\n",
    "        if item['entity'] == 'B-LOC' or item['entity'] == 'I-LOC':\n",
    "            loc_count += 1\n",
    "        elif item['entity'] == 'B-ORG' or item['entity'] == 'I-ORG':\n",
    "            org_count += 1\n",
    "        elif item['entity'] == 'B-PER' or item['entity'] == 'I-PER':\n",
    "            per_count += 1\n",
    "        elif item['entity'] == 'B-MISC' or item['entity'] == 'I-MISC':\n",
    "            misc_count += 1\n",
    "    return [loc_count, org_count, per_count, misc_count] #dim: (num_samples, 4)\n",
    "\n",
    "train_ner = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_ner.append(get_ner(text))\n",
    "train_ner = np.array(train_ner)\n",
    "\n",
    "val_ner = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_ner.append(get_ner(text))\n",
    "val_ner = np.array(val_ner)\n",
    "\n",
    "test_ner = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_ner.append(get_ner(text))\n",
    "test_ner = np.array(test_ner)\n",
    "print('train ner shape: ', train_ner.shape) #shape: (num_samples, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c68d2aa-dac0-4eec-8ec8-43aacd347712",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_punc = train_punc.reshape(-1,1)\n",
    "val_punc = val_punc.reshape(-1,1)\n",
    "test_punc = test_punc.reshape(-1,1)\n",
    "\n",
    "train_capital = train_capital.reshape(-1,1)\n",
    "val_capital = val_capital.reshape(-1,1)\n",
    "test_capital = test_capital.reshape(-1,1)\n",
    "\n",
    "train_sentiment = train_sentiment.reshape(-1,1)\n",
    "val_sentiment = val_sentiment.reshape(-1,1)\n",
    "test_sentiment = test_sentiment.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d432e9b1-9d1c-4330-a260-c534b20357f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features shape:  (17766, 12)\n",
      "[12 13  1  8 15  4 11 13  0  1  0  1]\n"
     ]
    }
   ],
   "source": [
    "#concatenate all features\n",
    "train_features = np.concatenate((train_punc, train_capital,train_sentiment, train_pos, train_ner), axis=1) #dim: (num_samples, 11)\n",
    "val_features = np.concatenate((val_punc, val_capital,val_sentiment, val_pos, val_ner), axis=1) #dim: (num_samples, 11)\n",
    "test_features = np.concatenate((test_punc, test_capital,test_sentiment, test_pos, test_ner), axis=1) #dim: (num_samples, 11)\n",
    "print('train features shape: ', train_features.shape) #shape: (num_samples, 11)\n",
    "print(train_features[0])\n",
    "#concatenate all features\n",
    "# train_features = np.concatenate((train_pos, train_ner), axis=1) #dim: (num_samples, 11)\n",
    "# val_features = np.concatenate((val_pos, val_ner), axis=1) #dim: (num_samples, 11)\n",
    "# test_features = np.concatenate((test_pos, test_ner), axis=1) #dim: (num_samples, 11)\n",
    "# print('train features shape: ', train_features.shape) #shape: (num_samples, 11)\n",
    "# print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c55bdc50-85cb-4bfd-b151-0df3602d29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save these features in a file\n",
    "np.save('train_features.npy', train_features)\n",
    "np.save('val_features.npy', val_features)\n",
    "np.save('test_features.npy', test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcf71b-28cf-4227-8593-bc0946540a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60687fc5-48dd-4f13-a45f-5232dbf168e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# # Set the number of components you want to keep\n",
    "# n_components = 15\n",
    "# # Fit PCA on the validation embeddings and transform them\n",
    "# pca = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f722eac-0bc5-481f-aa16-be080b667bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings_pca = pca.fit_transform(train_embeddings)\n",
    "# print('train embeddings pca shape: ', train_embeddings_pca.shape) #shape: (num_samples, n_components)\n",
    "\n",
    "# val_embeddings_pca = pca.transform(val_embeddings)\n",
    "# print('validation embeddings pca shape: ', val_embeddings_pca.shape) #shape: (num_samples, n_components)\n",
    "\n",
    "# test_embeddings_pca = pca.transform(test_embeddings)\n",
    "# print('test embeddings pca shape: ', test_embeddings_pca.shape) #shape: (num_samples, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47bab456-c4e9-4024-9269-0997bbc96440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pysentimiento import create_analyzer\n",
    "# analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "# text = \"Este es un ejemplo de texto con sentimiento.\"\n",
    "\n",
    "# result = analyzer.predict(text)\n",
    "\n",
    "# pos_prob = result.prob_pos\n",
    "# neg_prob = result.prob_neg\n",
    "# neu_prob = result.prob_neu\n",
    "\n",
    "# print(\"Positive Probability:\", pos_prob)\n",
    "# print(\"Negative Probability:\", neg_prob)\n",
    "# print(\"Neutral Probability:\", neu_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f4c8fe8d-c6d0-4c9a-aa85-04ef212b95d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0497, -0.4041, -0.3824,  ...,  0.0385,  0.6834,  0.2987],\n",
      "        [ 0.0174,  0.5261, -0.3379,  ..., -0.5308,  0.4677,  0.5002],\n",
      "        [ 0.2951,  0.7370,  0.2409,  ..., -1.1159,  0.2275,  0.1440],\n",
      "        ...,\n",
      "        [ 0.5721,  0.0393,  0.3432,  ..., -0.1891,  0.1854,  0.8616],\n",
      "        [ 0.3103,  0.5615,  0.0251,  ..., -1.0129,  0.2394,  0.2624],\n",
      "        [ 0.3008,  0.5643,  0.3183,  ..., -0.9900,  0.0948,  0.5841]])\n",
      "tensor([3, 1, 5, 5, 3, 0, 0, 5, 2, 0, 4, 1, 4, 0, 2, 0, 4, 4, 1, 0, 0, 4, 1, 5,\n",
      "        0, 0, 4, 3, 1, 0, 4, 5])\n",
      "tensor([[ 8, 14,  2,  8, 13,  4, 12, 14,  2, 14,  0,  0],\n",
      "        [11,  3,  2,  5, 17, 10, 10, 13,  0,  0,  0,  0],\n",
      "        [ 7,  4,  5, 10,  9,  5,  7, 10,  0,  0,  0,  0],\n",
      "        [ 8,  6,  3,  0, 23,  9, 17, 13,  0,  0,  0,  1],\n",
      "        [ 8,  8,  1,  2, 13, 13,  6, 13,  0,  0,  0,  0],\n",
      "        [ 7,  4,  4, 12, 22,  8, 14,  9,  0,  0,  0,  0],\n",
      "        [ 6,  2,  4,  5, 13,  5,  6, 10,  0,  0,  0,  0],\n",
      "        [10, 15,  3,  3, 14,  8, 15, 14,  3,  0, 14,  0],\n",
      "        [10,  5,  5,  8, 18, 11, 10, 12,  0,  0,  0,  0],\n",
      "        [ 6,  4,  3,  6, 16, 12,  9, 10,  0,  0,  1,  0],\n",
      "        [ 4,  5,  1,  1,  1,  4,  1,  1,  0,  0,  0,  0],\n",
      "        [ 8,  7,  5,  7, 14,  4,  6,  9,  0,  0,  0,  0],\n",
      "        [ 3,  2,  5,  0,  4,  4,  2,  2,  0,  0,  0,  0],\n",
      "        [ 8,  4,  5,  7, 10,  9,  4,  8,  0,  0,  0,  0],\n",
      "        [ 2,  2,  3,  2,  2,  4,  1,  2,  0,  0,  0,  2],\n",
      "        [ 8, 12,  4,  8, 16, 11, 16, 13,  0,  7,  0,  0],\n",
      "        [ 8,  9,  4, 10, 12,  1,  4,  8,  0,  0,  0,  0],\n",
      "        [ 4,  9,  3,  2,  8,  0,  4,  3,  0,  0,  0,  1],\n",
      "        [ 7, 27,  3,  4, 14,  3, 19, 18,  0, 17,  0, 16],\n",
      "        [ 9,  6,  2, 15, 20,  7, 14, 12,  0,  1,  0,  1],\n",
      "        [ 9, 82,  4,  8, 14,  5, 12, 15,  0,  7,  0,  5],\n",
      "        [ 8, 12,  4,  5, 19,  6, 13, 14,  0,  7,  6,  0],\n",
      "        [ 3,  0,  3,  1,  5,  3,  4,  2,  0,  0,  0,  1],\n",
      "        [ 4,  4,  3,  4, 12,  5,  8, 12,  0,  0,  0,  0],\n",
      "        [10, 15,  5,  2, 13,  6,  5,  8,  2,  3,  0,  0],\n",
      "        [ 2,  8,  5,  2,  7,  1,  3,  6,  0,  0,  0,  4],\n",
      "        [ 5,  5,  5,  1,  4,  1,  4,  3,  0,  2,  2,  1],\n",
      "        [ 9,  8,  5,  7, 10,  4,  3, 10,  0,  0,  0,  0],\n",
      "        [ 5,  8,  4,  9, 18,  8, 12, 16,  0,  5,  0,  0],\n",
      "        [ 3,  2,  3,  1,  6,  2,  3,  4,  0,  0,  0,  0],\n",
      "        [ 7,  8,  4, 10, 12,  5,  4, 10,  0,  0,  0,  0],\n",
      "        [ 6,  4,  5,  8,  9,  5,  4, 11,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_embeddings), torch.tensor(train_labels), torch.tensor(train_features))\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(val_embeddings), torch.tensor(val_labels), torch.tensor(val_features))\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_embeddings), torch.tensor(test_labels), torch.tensor(test_features))\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for embeddings, labels, f in train_loader:\n",
    "    print(embeddings)\n",
    "    print(labels)\n",
    "    print(f)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a656e10f-25e2-4afa-9299-96ef28cce2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define neural network architecture\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# #create a neural network to use the embeddings and do classification\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "#         self.dropout1 = nn.Dropout(0.1)\n",
    "#         self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # out = F.relu(self.bn1(self.fc1(x)))\n",
    "#         out = F.relu(self.fc1(x))\n",
    "#         out = self.dropout1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "    \n",
    "# # Hyperparameters\n",
    "# input_size = 768\n",
    "# hidden_size = 128\n",
    "# num_classes = 2\n",
    "# num_epochs = 20\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "77a3db57-1027-4a3b-91e0-1b8fb625f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#create a neural network to use the embeddings and do classification\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1) \n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2+12, num_classes) #11 is the number of features\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, f):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = torch.cat((out, f), dim=1)    \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "# Hyperparameters\n",
    "input_size = 768\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 38\n",
    "num_classes = 6\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "16088ff5-22ca-4c06-8adc-dc296bca8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model from the neural network\n",
    "# model = Net(input_size, hidden_size, num_classes).to(device)\n",
    "model = Net(input_size, hidden_size1, hidden_size2, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9434cde1-daaf-495b-9bcc-436b37d8ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# best_val_acc = 0.0\n",
    "# total_step = len(train_loader)\n",
    "# half_epoch_step = total_step // 2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, (embeddings, labels) in tqdm(enumerate(train_loader), total=total_step, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "#         # Move tensors to the configured device\n",
    "#         embeddings = embeddings.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(embeddings)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         # Print loss every half epoch\n",
    "#         if (i+1) % half_epoch_step == 0:\n",
    "#             avg_loss = running_loss / half_epoch_step\n",
    "#             print(f\"Epoch {epoch+1}/{num_epochs} Loss after {i+1} batches: {avg_loss:.4f}\")\n",
    "#             running_loss = 0.0\n",
    "            \n",
    "#     # Validate the model\n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for embeddings, labels in val_loader:\n",
    "#             embeddings = embeddings.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(embeddings)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         # Print validation stats\n",
    "#         val_acc = 100 * correct / total\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_acc:.2f} %')\n",
    "\n",
    "#         # Save the model if the validation accuracy is better than the previous best\n",
    "#         if val_acc > best_val_acc:\n",
    "#             best_val_acc = val_acc\n",
    "#             torch.save(model.state_dict(), 'best_model.pt')\n",
    "#             print(f'Saved model with validation accuracy: {best_val_acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2692e33c-1f22-4548-8ef1-e16e1411ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  58%|██████████████▌          | 324/556 [00:01<00:00, 266.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss after 278 batches: 1.6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|█████████████████████████| 556/556 [00:01<00:00, 279.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss after 556 batches: 1.3845\n",
      "Epoch 1/20 Validation Accuracy: 42.48 %, Validation Loss: 1.3152\n",
      "Saved model with validation loss: 1.3152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  60%|███████████████          | 334/556 [00:01<00:00, 308.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Loss after 278 batches: 1.2823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|█████████████████████████| 556/556 [00:01<00:00, 308.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Loss after 556 batches: 1.2456\n",
      "Epoch 2/20 Validation Accuracy: 44.96 %, Validation Loss: 1.2302\n",
      "Saved model with validation loss: 1.2302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:  54%|█████████████▍           | 300/556 [00:01<00:00, 269.97batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss after 278 batches: 1.1896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|█████████████████████████| 556/556 [00:02<00:00, 270.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss after 556 batches: 1.1703\n",
      "Epoch 3/20 Validation Accuracy: 46.08 %, Validation Loss: 1.2254\n",
      "Saved model with validation loss: 1.2254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:  56%|█████████████▉           | 311/556 [00:01<00:01, 228.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Loss after 278 batches: 1.1137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|█████████████████████████| 556/556 [00:02<00:00, 246.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Loss after 556 batches: 1.1287\n",
      "Epoch 4/20 Validation Accuracy: 46.58 %, Validation Loss: 1.1662\n",
      "Saved model with validation loss: 1.1662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:  59%|██████████████▊          | 330/556 [00:01<00:00, 272.87batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Loss after 278 batches: 1.0630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|█████████████████████████| 556/556 [00:02<00:00, 277.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Loss after 556 batches: 1.0712\n",
      "Epoch 5/20 Validation Accuracy: 47.09 %, Validation Loss: 1.1909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:  55%|█████████████▋           | 304/556 [00:01<00:00, 269.85batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Loss after 278 batches: 1.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|█████████████████████████| 556/556 [00:02<00:00, 257.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Loss after 556 batches: 1.0336\n",
      "Epoch 6/20 Validation Accuracy: 47.65 %, Validation Loss: 1.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:  57%|██████████████▏          | 316/556 [00:01<00:00, 265.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Loss after 278 batches: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|█████████████████████████| 556/556 [00:02<00:00, 256.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Loss after 556 batches: 0.9854\n",
      "Epoch 7/20 Validation Accuracy: 48.15 %, Validation Loss: 1.1861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:  57%|██████████████▏          | 315/556 [00:01<00:00, 278.69batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Loss after 278 batches: 0.9204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|█████████████████████████| 556/556 [00:02<00:00, 267.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Loss after 556 batches: 0.9471\n",
      "Epoch 8/20 Validation Accuracy: 48.10 %, Validation Loss: 1.2235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:  57%|██████████████▏          | 315/556 [00:01<00:00, 282.56batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Loss after 278 batches: 0.8769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|█████████████████████████| 556/556 [00:02<00:00, 277.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Loss after 556 batches: 0.9026\n",
      "Epoch 9/20 Validation Accuracy: 45.42 %, Validation Loss: 1.2733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:  63%|███████████████         | 350/556 [00:01<00:00, 336.58batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Loss after 278 batches: 0.8233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|████████████████████████| 556/556 [00:02<00:00, 270.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Loss after 556 batches: 0.8784\n",
      "Epoch 10/20 Validation Accuracy: 47.19 %, Validation Loss: 1.2710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:  58%|█████████████▊          | 321/556 [00:01<00:01, 231.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Loss after 278 batches: 0.7881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|████████████████████████| 556/556 [00:02<00:00, 243.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Loss after 556 batches: 0.8213\n",
      "Epoch 11/20 Validation Accuracy: 45.22 %, Validation Loss: 1.3727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:  58%|██████████████          | 325/556 [00:01<00:00, 261.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Loss after 278 batches: 0.7604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|████████████████████████| 556/556 [00:02<00:00, 240.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Loss after 556 batches: 0.7820\n",
      "Epoch 12/20 Validation Accuracy: 46.84 %, Validation Loss: 1.3908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:  60%|██████████████▍         | 335/556 [00:01<00:00, 265.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Loss after 278 batches: 0.7147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|████████████████████████| 556/556 [00:02<00:00, 243.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Loss after 556 batches: 0.7347\n",
      "Epoch 13/20 Validation Accuracy: 46.03 %, Validation Loss: 1.4354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:  61%|██████████████▋         | 339/556 [00:01<00:00, 304.64batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Loss after 278 batches: 0.6668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|████████████████████████| 556/556 [00:02<00:00, 261.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Loss after 556 batches: 0.7117\n",
      "Epoch 14/20 Validation Accuracy: 46.38 %, Validation Loss: 1.4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:  58%|█████████████▉          | 323/556 [00:01<00:00, 260.05batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Loss after 278 batches: 0.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|████████████████████████| 556/556 [00:02<00:00, 244.24batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Loss after 556 batches: 0.6749\n",
      "Epoch 15/20 Validation Accuracy: 46.94 %, Validation Loss: 1.5611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:  58%|█████████████▉          | 322/556 [00:01<00:00, 274.85batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Loss after 278 batches: 0.5998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|████████████████████████| 556/556 [00:02<00:00, 265.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Loss after 556 batches: 0.6407\n",
      "Epoch 16/20 Validation Accuracy: 45.87 %, Validation Loss: 1.6362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  56%|█████████████▍          | 311/556 [00:01<00:00, 264.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Loss after 278 batches: 0.5687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|████████████████████████| 556/556 [00:02<00:00, 275.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Loss after 556 batches: 0.6140\n",
      "Epoch 17/20 Validation Accuracy: 47.14 %, Validation Loss: 1.6332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  55%|█████████████           | 304/556 [00:01<00:00, 266.98batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Loss after 278 batches: 0.5521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|████████████████████████| 556/556 [00:02<00:00, 254.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Loss after 556 batches: 0.5710\n",
      "Epoch 18/20 Validation Accuracy: 46.53 %, Validation Loss: 1.6907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:  59%|██████████████          | 327/556 [00:01<00:00, 234.48batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Loss after 278 batches: 0.5137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|████████████████████████| 556/556 [00:02<00:00, 231.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Loss after 556 batches: 0.5633\n",
      "Epoch 19/20 Validation Accuracy: 45.97 %, Validation Loss: 1.7297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:  54%|████████████▉           | 299/556 [00:01<00:00, 271.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Loss after 278 batches: 0.4913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|████████████████████████| 556/556 [00:02<00:00, 262.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Loss after 556 batches: 0.5415\n",
      "Epoch 20/20 Validation Accuracy: 45.32 %, Validation Loss: 1.8432\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_val_loss = float('inf') # initialize best validation loss to infinity\n",
    "total_step = len(train_loader)\n",
    "half_epoch_step = total_step // 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (embeddings, labels, f) in tqdm(enumerate(train_loader), total=total_step, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        # Move tensors to the configured device\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        f = f.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeddings, f)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print loss every half epoch\n",
    "        if (i+1) % half_epoch_step == 0:\n",
    "            avg_loss = running_loss / half_epoch_step\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} Loss after {i+1} batches: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # Validate the model\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for embeddings, labels, f in val_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "            f = f.to(device)\n",
    "            outputs = model(embeddings, f)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print validation stats\n",
    "        val_acc = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_acc:.2f} %, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save the model if the validation loss is better than the previous best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'Saved model with validation loss: {best_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d92a668c-3515-4613-a91c-34f71d9a39dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 69/69 [00:00<00:00, 559.19it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    for embeddings, labels, f in tqdm(test_loader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        f = f.to(device)\n",
    "        outputs = model(embeddings,f)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    #generate classification report\n",
    "    test_report = classification_report(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0baedfa6-5d2a-4b81-8a4f-0ff6003a48d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.51      0.48       317\n",
      "           1       0.31      0.40      0.35       354\n",
      "           2       0.37      0.24      0.29       370\n",
      "           3       0.47      0.50      0.49       378\n",
      "           4       0.39      0.39      0.39       369\n",
      "           5       0.75      0.70      0.73       406\n",
      "\n",
      "    accuracy                           0.46      2194\n",
      "   macro avg       0.46      0.46      0.45      2194\n",
      "weighted avg       0.46      0.46      0.46      2194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc0a77-d4f4-49d9-b9b7-e649f9ec43c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "pyt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
