{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ab5284-41b9-4d0f-b529-2fa7b29d9c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0cc42b-5960-44a4-b0a0-4a251e5e532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a39071-e43f-46a6-893f-ce219f6dd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240511a7-0cd8-4abc-9d00-bf68a4ba7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 25\n",
    "# random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724952d7-1d42-4f71-acfc-dbf71b58217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                               text      label\n",
      "0   5464  Entrada en vigor. La presente Directiva entrar...      human\n",
      "1  30129  Preguntas: 1. ¿Cuáles son los principales argu...  generated\n",
      "2  19553  ¿Desea algo? Póngame una caja de madera. ¿Qué ...  generated\n",
      "3  13005  @victor28088 1665 Tweets no originales, que as...      human\n",
      "4  16919  De pequeño Dios me dio a elegir entre tener un...      human\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('datasets/subtask_1/es/train.tsv',sep='\\t')\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "883429df-a252-41d8-9e6a-cf224bf9951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:  25969\n",
      "validation data size:  2886\n",
      "test data size:  3207\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data_texts = train_data['text'].to_list()\n",
    "train_data_labels = train_data['label'].to_list()\n",
    "train_data_labels = [0 if x=='human' else 1 for x in train_data_labels]\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_data_texts, train_data_labels, test_size=0.1, random_state=25)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=25)\n",
    "print('train data size: ', len(train_texts))\n",
    "print('validation data size: ', len(val_texts))\n",
    "print('test data size: ', len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246a21d5-d834-4052-8a29-7c1d4325e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configurations\n",
      "\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "bert_model = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\").to(device)\n",
    "print(\"Model Configurations\")\n",
    "print()\n",
    "print(bert_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05431d70-74a3-49dc-b3e5-7cbf6b007c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25969/25969 [04:07<00:00, 104.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape:  (25969, 768)\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    # Tokenize input text\n",
    "    encoded_input = bert_tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "    #get bert embeddings\n",
    "    with torch.no_grad():\n",
    "        bert_output = bert_model(**encoded_input)\n",
    "    bert_embeddings = bert_output.last_hidden_state[:,0,:].cpu().numpy()\n",
    "    return bert_embeddings\n",
    "\n",
    "#get train embeddings\n",
    "train_embeddings = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_embeddings.append(get_bert_embeddings(text))\n",
    "train_embeddings = np.array(train_embeddings)\n",
    "train_embeddings = np.squeeze(train_embeddings, axis=1)\n",
    "print('train embeddings shape: ', train_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7050edb3-44bd-4884-8ac3-53bb6ea4c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2886/2886 [00:27<00:00, 106.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation embeddings shape:  (2886, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3207/3207 [00:30<00:00, 106.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test embeddings shape:  (3207, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#get validation embeddings\n",
    "val_embeddings = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_embeddings.append(get_bert_embeddings(text))\n",
    "val_embeddings = np.array(val_embeddings)\n",
    "val_embeddings = np.squeeze(val_embeddings, axis=1)\n",
    "print('validation embeddings shape: ', val_embeddings.shape) #shape: (num_samples, 1, 768)\n",
    "\n",
    "\n",
    "#get test embeddings\n",
    "test_embeddings = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_embeddings.append(get_bert_embeddings(text))\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "test_embeddings = np.squeeze(test_embeddings, axis=1)\n",
    "print('test embeddings shape: ', test_embeddings.shape) #shape: (num_samples, 1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9983d3f9-9d90-4452-aace-f8b6f436a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train punc shape:  (25969,)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def count_punctuations(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return count\n",
    "\n",
    "train_punc = []\n",
    "for text in train_texts:\n",
    "    train_punc.append(count_punctuations(text))\n",
    "train_punc = np.array(train_punc)\n",
    "\n",
    "val_punc = []\n",
    "for text in val_texts:\n",
    "    val_punc.append(count_punctuations(text))\n",
    "val_punc = np.array(val_punc)\n",
    "\n",
    "test_punc = []\n",
    "for text in test_texts:\n",
    "    test_punc.append(count_punctuations(text))\n",
    "test_punc = np.array(test_punc)\n",
    "print('train punc shape: ', train_punc.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06d55ee-e255-4047-b08e-e2cb194a37f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train capital shape:  (25969,)\n"
     ]
    }
   ],
   "source": [
    "def count_capital_letters(text):\n",
    "    count = sum([1 for char in text if char.isupper()])\n",
    "    return count\n",
    "\n",
    "train_capital = []\n",
    "for text in train_texts:\n",
    "    train_capital.append(count_capital_letters(text))\n",
    "train_capital = np.array(train_capital)\n",
    "\n",
    "val_capital = []\n",
    "for text in val_texts:\n",
    "    val_capital.append(count_capital_letters(text))\n",
    "val_capital = np.array(val_capital)\n",
    "\n",
    "test_capital = []\n",
    "for text in test_texts:\n",
    "    test_capital.append(count_capital_letters(text))\n",
    "test_capital = np.array(test_capital)\n",
    "print('train capital shape: ', train_capital.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab44401c-5ed6-4441-93f3-c9170c4edf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform sentiment analysis on a spanish text\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5380762c-118f-4560-a620-b78aaf0bc5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 25969/25969 [19:34<00:00, 22.12it/s]\n",
      "100%|███████████████████████████████████████| 2886/2886 [02:10<00:00, 22.16it/s]\n",
      "100%|███████████████████████████████████████| 3207/3207 [02:25<00:00, 22.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentiment shape:  (25969,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    sentiment = sentiment_analysis(text)[0]['label']\n",
    "    return sentiment #dim: (num_samples, 1) range: [1,5]\n",
    "\n",
    "train_sentiment = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_sentiment.append(get_sentiment(text))\n",
    "train_sentiment = np.array(train_sentiment)\n",
    "\n",
    "val_sentiment = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_sentiment.append(get_sentiment(text))\n",
    "val_sentiment = np.array(val_sentiment)\n",
    "\n",
    "test_sentiment = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_sentiment.append(get_sentiment(text))\n",
    "test_sentiment = np.array(test_sentiment)\n",
    "print('train sentiment shape: ', train_sentiment.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a2ac049-cc43-4c2a-860d-3c09d742953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25969/25969 [03:51<00:00, 112.35it/s]\n",
      "100%|██████████████████████████████████████| 2886/2886 [00:25<00:00, 115.10it/s]\n",
      "100%|██████████████████████████████████████| 3207/3207 [00:28<00:00, 114.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pos shape:  (25969, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "#function to get pos tags for each category\n",
    "def get_pos(text):\n",
    "    doc = nlp(text)\n",
    "    adj_count = 0\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adp_count = 0\n",
    "    det_count = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            adj_count += 1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun_count += 1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb_count += 1\n",
    "        elif token.pos_ == 'ADP':\n",
    "            adp_count += 1\n",
    "        elif token.pos_ == 'DET':\n",
    "            det_count += 1\n",
    "    return [adj_count, noun_count, verb_count, adp_count, det_count] #dim: (num_samples, 5) \n",
    "\n",
    "train_pos = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_pos.append(get_pos(text))\n",
    "train_pos = np.array(train_pos)\n",
    "\n",
    "val_pos = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_pos.append(get_pos(text))\n",
    "val_pos = np.array(val_pos)\n",
    "\n",
    "test_pos = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_pos.append(get_pos(text))\n",
    "test_pos = np.array(test_pos)\n",
    "print('train pos shape: ', train_pos.shape) #shape: (num_samples, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67b74ff8-322c-4800-a2b1-960d2c618740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "ner_analysis = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d1fe338-ffd1-438f-974e-853411dc875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 25969/25969 [18:02<00:00, 23.98it/s]\n",
      "100%|███████████████████████████████████████| 2886/2886 [01:59<00:00, 24.16it/s]\n",
      "100%|███████████████████████████████████████| 3207/3207 [02:13<00:00, 24.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ner shape:  (25969, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner = ner_analysis(text)\n",
    "    loc_count = 0\n",
    "    org_count = 0\n",
    "    per_count = 0\n",
    "    misc_count = 0\n",
    "    for item in ner:\n",
    "        if item['entity'] == 'B-LOC' or item['entity'] == 'I-LOC':\n",
    "            loc_count += 1\n",
    "        elif item['entity'] == 'B-ORG' or item['entity'] == 'I-ORG':\n",
    "            org_count += 1\n",
    "        elif item['entity'] == 'B-PER' or item['entity'] == 'I-PER':\n",
    "            per_count += 1\n",
    "        elif item['entity'] == 'B-MISC' or item['entity'] == 'I-MISC':\n",
    "            misc_count += 1\n",
    "    return [loc_count, org_count, per_count, misc_count] #dim: (num_samples, 4)\n",
    "\n",
    "train_ner = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_ner.append(get_ner(text))\n",
    "train_ner = np.array(train_ner)\n",
    "\n",
    "val_ner = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_ner.append(get_ner(text))\n",
    "val_ner = np.array(val_ner)\n",
    "\n",
    "test_ner = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_ner.append(get_ner(text))\n",
    "test_ner = np.array(test_ner)\n",
    "print('train ner shape: ', train_ner.shape) #shape: (num_samples, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c68d2aa-dac0-4eec-8ec8-43aacd347712",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_punc = train_punc.reshape(-1,1)\n",
    "val_punc = val_punc.reshape(-1,1)\n",
    "test_punc = test_punc.reshape(-1,1)\n",
    "\n",
    "train_capital = train_capital.reshape(-1,1)\n",
    "val_capital = val_capital.reshape(-1,1)\n",
    "test_capital = test_capital.reshape(-1,1)\n",
    "\n",
    "train_sentiment = train_sentiment.reshape(-1,1)\n",
    "val_sentiment = val_sentiment.reshape(-1,1)\n",
    "test_sentiment = test_sentiment.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d432e9b1-9d1c-4330-a260-c534b20357f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features shape:  (25969, 9)\n",
      "[1 1 5 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#concatenate all features\n",
    "# train_features = np.concatenate((train_punc, train_capital, train_pos, train_ner), axis=1) #dim: (num_samples, 11)\n",
    "# val_features = np.concatenate((val_punc, val_capital, val_pos, val_ner), axis=1) #dim: (num_samples, 11)\n",
    "# test_features = np.concatenate((test_punc, test_capital, test_pos, test_ner), axis=1) #dim: (num_samples, 11)\n",
    "# print('train features shape: ', train_features.shape) #shape: (num_samples, 11)\n",
    "# print(train_features[0])\n",
    "#concatenate all features\n",
    "train_features = np.concatenate((train_pos, train_ner), axis=1) #dim: (num_samples, 11)\n",
    "val_features = np.concatenate((val_pos, val_ner), axis=1) #dim: (num_samples, 11)\n",
    "test_features = np.concatenate((test_pos, test_ner), axis=1) #dim: (num_samples, 11)\n",
    "print('train features shape: ', train_features.shape) #shape: (num_samples, 11)\n",
    "print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c55bdc50-85cb-4bfd-b151-0df3602d29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save these features in a file\n",
    "np.save('train_features.npy', train_features)\n",
    "np.save('val_features.npy', val_features)\n",
    "np.save('test_features.npy', test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcf71b-28cf-4227-8593-bc0946540a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "60687fc5-48dd-4f13-a45f-5232dbf168e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# # Set the number of components you want to keep\n",
    "# n_components = 15\n",
    "# # Fit PCA on the validation embeddings and transform them\n",
    "# pca = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1f722eac-0bc5-481f-aa16-be080b667bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings_pca = pca.fit_transform(train_embeddings)\n",
    "# print('train embeddings pca shape: ', train_embeddings_pca.shape) #shape: (num_samples, n_components)\n",
    "\n",
    "# val_embeddings_pca = pca.transform(val_embeddings)\n",
    "# print('validation embeddings pca shape: ', val_embeddings_pca.shape) #shape: (num_samples, n_components)\n",
    "\n",
    "# test_embeddings_pca = pca.transform(test_embeddings)\n",
    "# print('test embeddings pca shape: ', test_embeddings_pca.shape) #shape: (num_samples, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "47bab456-c4e9-4024-9269-0997bbc96440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pysentimiento import create_analyzer\n",
    "# analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "# text = \"Este es un ejemplo de texto con sentimiento.\"\n",
    "\n",
    "# result = analyzer.predict(text)\n",
    "\n",
    "# pos_prob = result.prob_pos\n",
    "# neg_prob = result.prob_neg\n",
    "# neu_prob = result.prob_neu\n",
    "\n",
    "# print(\"Positive Probability:\", pos_prob)\n",
    "# print(\"Negative Probability:\", neg_prob)\n",
    "# print(\"Neutral Probability:\", neu_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f4c8fe8d-c6d0-4c9a-aa85-04ef212b95d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 9])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_embeddings), torch.tensor(train_labels), torch.tensor(train_features))\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(val_embeddings), torch.tensor(val_labels), torch.tensor(val_features))\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_embeddings), torch.tensor(test_labels), torch.tensor(test_features))\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for embeddings, labels, f in train_loader:\n",
    "    print(embeddings.shape)\n",
    "    print(labels.shape)\n",
    "    print(f.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a656e10f-25e2-4afa-9299-96ef28cce2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define neural network architecture\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# #create a neural network to use the embeddings and do classification\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "#         self.dropout1 = nn.Dropout(0.1)\n",
    "#         self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # out = F.relu(self.bn1(self.fc1(x)))\n",
    "#         out = F.relu(self.fc1(x))\n",
    "#         out = self.dropout1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "    \n",
    "# # Hyperparameters\n",
    "# input_size = 768\n",
    "# hidden_size = 128\n",
    "# num_classes = 2\n",
    "# num_epochs = 20\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "77a3db57-1027-4a3b-91e0-1b8fb625f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#create a neural network to use the embeddings and do classification\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1) \n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2+9, num_classes) #11 is the number of features\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, f):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = torch.cat((out, f), dim=1)    \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "# Hyperparameters\n",
    "input_size = 768\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 21\n",
    "num_classes = 2\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "16088ff5-22ca-4c06-8adc-dc296bca8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model from the neural network\n",
    "# model = Net(input_size, hidden_size, num_classes).to(device)\n",
    "model = Net(input_size, hidden_size1, hidden_size2, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9434cde1-daaf-495b-9bcc-436b37d8ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# best_val_acc = 0.0\n",
    "# total_step = len(train_loader)\n",
    "# half_epoch_step = total_step // 2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, (embeddings, labels) in tqdm(enumerate(train_loader), total=total_step, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "#         # Move tensors to the configured device\n",
    "#         embeddings = embeddings.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(embeddings)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         # Print loss every half epoch\n",
    "#         if (i+1) % half_epoch_step == 0:\n",
    "#             avg_loss = running_loss / half_epoch_step\n",
    "#             print(f\"Epoch {epoch+1}/{num_epochs} Loss after {i+1} batches: {avg_loss:.4f}\")\n",
    "#             running_loss = 0.0\n",
    "            \n",
    "#     # Validate the model\n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for embeddings, labels in val_loader:\n",
    "#             embeddings = embeddings.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(embeddings)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         # Print validation stats\n",
    "#         val_acc = 100 * correct / total\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_acc:.2f} %')\n",
    "\n",
    "#         # Save the model if the validation accuracy is better than the previous best\n",
    "#         if val_acc > best_val_acc:\n",
    "#             best_val_acc = val_acc\n",
    "#             torch.save(model.state_dict(), 'best_model.pt')\n",
    "#             print(f'Saved model with validation accuracy: {best_val_acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2692e33c-1f22-4548-8ef1-e16e1411ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  55%|█████████████▊           | 447/812 [00:01<00:00, 388.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss after 406 batches: 0.5094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|█████████████████████████| 812/812 [00:02<00:00, 377.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss after 812 batches: 0.3753\n",
      "Epoch 1/20 Validation Accuracy: 83.37 %, Validation Loss: 0.3613\n",
      "Saved model with validation loss: 0.3613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  59%|██████████████▊          | 480/812 [00:01<00:00, 397.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Loss after 406 batches: 0.3429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|█████████████████████████| 812/812 [00:02<00:00, 394.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Loss after 812 batches: 0.3283\n",
      "Epoch 2/20 Validation Accuracy: 84.86 %, Validation Loss: 0.3502\n",
      "Saved model with validation loss: 0.3502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:  59%|██████████████▊          | 481/812 [00:01<00:00, 397.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss after 406 batches: 0.3231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|█████████████████████████| 812/812 [00:02<00:00, 396.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss after 812 batches: 0.3064\n",
      "Epoch 3/20 Validation Accuracy: 84.16 %, Validation Loss: 0.3482\n",
      "Saved model with validation loss: 0.3482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:  59%|██████████████▊          | 480/812 [00:01<00:00, 396.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Loss after 406 batches: 0.2962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|█████████████████████████| 812/812 [00:02<00:00, 396.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Loss after 812 batches: 0.2987\n",
      "Epoch 4/20 Validation Accuracy: 86.07 %, Validation Loss: 0.3213\n",
      "Saved model with validation loss: 0.3213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:  57%|██████████████▏          | 460/812 [00:01<00:00, 392.54batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Loss after 406 batches: 0.2781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|█████████████████████████| 812/812 [00:02<00:00, 384.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Loss after 812 batches: 0.2897\n",
      "Epoch 5/20 Validation Accuracy: 85.93 %, Validation Loss: 0.3164\n",
      "Saved model with validation loss: 0.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:  59%|██████████████▊          | 480/812 [00:01<00:00, 395.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Loss after 406 batches: 0.2777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|█████████████████████████| 812/812 [00:02<00:00, 396.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Loss after 812 batches: 0.2675\n",
      "Epoch 6/20 Validation Accuracy: 85.93 %, Validation Loss: 0.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:  60%|██████████████▉          | 484/812 [00:01<00:00, 398.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Loss after 406 batches: 0.2578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|█████████████████████████| 812/812 [00:02<00:00, 397.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Loss after 812 batches: 0.2643\n",
      "Epoch 7/20 Validation Accuracy: 86.45 %, Validation Loss: 0.3217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:  59%|██████████████▊          | 483/812 [00:01<00:00, 398.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Loss after 406 batches: 0.2469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|█████████████████████████| 812/812 [00:02<00:00, 397.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Loss after 812 batches: 0.2510\n",
      "Epoch 8/20 Validation Accuracy: 86.45 %, Validation Loss: 0.3161\n",
      "Saved model with validation loss: 0.3161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:  59%|██████████████▊          | 480/812 [00:01<00:00, 397.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Loss after 406 batches: 0.2401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|█████████████████████████| 812/812 [00:02<00:00, 396.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Loss after 812 batches: 0.2402\n",
      "Epoch 9/20 Validation Accuracy: 86.11 %, Validation Loss: 0.3220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:  59%|██████████████▏         | 480/812 [00:01<00:00, 396.80batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Loss after 406 batches: 0.2284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|████████████████████████| 812/812 [00:02<00:00, 396.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Loss after 812 batches: 0.2371\n",
      "Epoch 10/20 Validation Accuracy: 86.11 %, Validation Loss: 0.3233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:  59%|██████████████▏         | 480/812 [00:01<00:00, 395.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Loss after 406 batches: 0.2199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|████████████████████████| 812/812 [00:02<00:00, 395.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Loss after 812 batches: 0.2272\n",
      "Epoch 11/20 Validation Accuracy: 86.11 %, Validation Loss: 0.3363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:  59%|██████████████▏         | 480/812 [00:01<00:00, 397.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Loss after 406 batches: 0.2162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|████████████████████████| 812/812 [00:02<00:00, 395.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Loss after 812 batches: 0.2167\n",
      "Epoch 12/20 Validation Accuracy: 86.11 %, Validation Loss: 0.3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:  59%|██████████████▏         | 481/812 [00:01<00:00, 396.57batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Loss after 406 batches: 0.1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|████████████████████████| 812/812 [00:02<00:00, 396.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Loss after 812 batches: 0.2126\n",
      "Epoch 13/20 Validation Accuracy: 85.90 %, Validation Loss: 0.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:  59%|██████████████▏         | 480/812 [00:01<00:00, 396.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Loss after 406 batches: 0.2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|████████████████████████| 812/812 [00:02<00:00, 395.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Loss after 812 batches: 0.2072\n",
      "Epoch 14/20 Validation Accuracy: 86.04 %, Validation Loss: 0.3358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:  59%|██████████████▏         | 480/812 [00:01<00:00, 396.11batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Loss after 406 batches: 0.1873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|████████████████████████| 812/812 [00:02<00:00, 395.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Loss after 812 batches: 0.1935\n",
      "Epoch 15/20 Validation Accuracy: 85.72 %, Validation Loss: 0.3479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:  59%|██████████████▏         | 478/812 [00:01<00:00, 395.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Loss after 406 batches: 0.1858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|████████████████████████| 812/812 [00:02<00:00, 394.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Loss after 812 batches: 0.1896\n",
      "Epoch 16/20 Validation Accuracy: 86.04 %, Validation Loss: 0.3678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  59%|██████████████▏         | 480/812 [00:01<00:00, 392.15batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Loss after 406 batches: 0.1808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|████████████████████████| 812/812 [00:02<00:00, 392.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Loss after 812 batches: 0.1822\n",
      "Epoch 17/20 Validation Accuracy: 85.72 %, Validation Loss: 0.3550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  59%|██████████████▏         | 479/812 [00:01<00:00, 393.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Loss after 406 batches: 0.1739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|████████████████████████| 812/812 [00:02<00:00, 393.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Loss after 812 batches: 0.1770\n",
      "Epoch 18/20 Validation Accuracy: 84.79 %, Validation Loss: 0.3685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:  59%|██████████████▏         | 480/812 [00:01<00:00, 395.89batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Loss after 406 batches: 0.1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|████████████████████████| 812/812 [00:02<00:00, 395.07batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Loss after 812 batches: 0.1785\n",
      "Epoch 19/20 Validation Accuracy: 86.17 %, Validation Loss: 0.3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:  59%|██████████████▏         | 480/812 [00:01<00:00, 395.11batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Loss after 406 batches: 0.1613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|████████████████████████| 812/812 [00:02<00:00, 395.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Loss after 812 batches: 0.1626\n",
      "Epoch 20/20 Validation Accuracy: 85.93 %, Validation Loss: 0.3703\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_val_loss = float('inf') # initialize best validation loss to infinity\n",
    "total_step = len(train_loader)\n",
    "half_epoch_step = total_step // 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (embeddings, labels, f) in tqdm(enumerate(train_loader), total=total_step, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        # Move tensors to the configured device\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        f = f.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeddings, f)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print loss every half epoch\n",
    "        if (i+1) % half_epoch_step == 0:\n",
    "            avg_loss = running_loss / half_epoch_step\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} Loss after {i+1} batches: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # Validate the model\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for embeddings, labels, f in val_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "            f = f.to(device)\n",
    "            outputs = model(embeddings, f)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print validation stats\n",
    "        val_acc = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_acc:.2f} %, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save the model if the validation loss is better than the previous best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'Saved model with validation loss: {best_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d92a668c-3515-4613-a91c-34f71d9a39dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 101/101 [00:00<00:00, 760.08it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    for embeddings, labels, f in tqdm(test_loader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        f = f.to(device)\n",
    "        outputs = model(embeddings,f)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    #generate classification report\n",
    "    test_report = classification_report(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6d2bd8f0-7b27-4d05-857f-448beba8089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86      1579\n",
      "           1       0.88      0.84      0.86      1628\n",
      "\n",
      "    accuracy                           0.86      3207\n",
      "   macro avg       0.86      0.86      0.86      3207\n",
      "weighted avg       0.86      0.86      0.86      3207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc11dcd2-b774-419f-a5b5-abe514f83d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86      1579\n",
      "           1       0.86      0.87      0.87      1628\n",
      "\n",
      "    accuracy                           0.87      3207\n",
      "   macro avg       0.87      0.87      0.87      3207\n",
      "weighted avg       0.87      0.87      0.87      3207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc0a77-d4f4-49d9-b9b7-e649f9ec43c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "pyt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
