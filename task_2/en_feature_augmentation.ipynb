{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ab5284-41b9-4d0f-b529-2fa7b29d9c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0cc42b-5960-44a4-b0a0-4a251e5e532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a39071-e43f-46a6-893f-ce219f6dd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240511a7-0cd8-4abc-9d00-bf68a4ba7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 25\n",
    "# random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724952d7-1d42-4f71-acfc-dbf71b58217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                               text label\n",
      "0   6239  It was not until many years later that it coul...     A\n",
      "1   9255  Users can then pin these images to their profi...     F\n",
      "2   1674  The best songs are those that I can sing along...     B\n",
      "3   5001  I found this book to be poorly written. It was...     D\n",
      "4  20779  Regulates the application of the EU tariff quo...     E\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('datasets/subtask_2/en/train.tsv',sep='\\t')\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883429df-a252-41d8-9e6a-cf224bf9951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:  18156\n",
      "validation data size:  2018\n",
      "test data size:  2242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data_texts = train_data['text'].to_list()\n",
    "train_data_labels = train_data['label'].to_list()\n",
    "train_data_labels = [ord(label)-ord('A') for label in train_data_labels]\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_data_texts, train_data_labels, test_size=0.1, random_state=25)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=25)\n",
    "print('train data size: ', len(train_texts))\n",
    "print('validation data size: ', len(val_texts))\n",
    "print('test data size: ', len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "246a21d5-d834-4052-8a29-7c1d4325e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configurations\n",
      "\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "print(\"Model Configurations\")\n",
    "print()\n",
    "print(bert_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05431d70-74a3-49dc-b3e5-7cbf6b007c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 18156/18156 [03:09<00:00, 95.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape:  (18156, 768)\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    # Tokenize input text\n",
    "    encoded_input = bert_tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "    #get bert embeddings\n",
    "    with torch.no_grad():\n",
    "        bert_output = bert_model(**encoded_input)\n",
    "    bert_embeddings = bert_output.last_hidden_state[:,0,:].cpu().numpy()\n",
    "    return bert_embeddings\n",
    "\n",
    "#get train embeddings\n",
    "train_embeddings = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_embeddings.append(get_bert_embeddings(text))\n",
    "train_embeddings = np.array(train_embeddings)\n",
    "train_embeddings = np.squeeze(train_embeddings, axis=1)\n",
    "print('train embeddings shape: ', train_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7050edb3-44bd-4884-8ac3-53bb6ea4c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2018/2018 [00:20<00:00, 99.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation embeddings shape:  (2018, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2242/2242 [00:22<00:00, 99.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test embeddings shape:  (2242, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#get validation embeddings\n",
    "val_embeddings = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_embeddings.append(get_bert_embeddings(text))\n",
    "val_embeddings = np.array(val_embeddings)\n",
    "val_embeddings = np.squeeze(val_embeddings, axis=1)\n",
    "print('validation embeddings shape: ', val_embeddings.shape) #shape: (num_samples, 1, 768)\n",
    "\n",
    "\n",
    "#get test embeddings\n",
    "test_embeddings = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_embeddings.append(get_bert_embeddings(text))\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "test_embeddings = np.squeeze(test_embeddings, axis=1)\n",
    "print('test embeddings shape: ', test_embeddings.shape) #shape: (num_samples, 1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9983d3f9-9d90-4452-aace-f8b6f436a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train punc shape:  (18156,)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def count_punctuations(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return count\n",
    "\n",
    "train_punc = []\n",
    "for text in train_texts:\n",
    "    train_punc.append(count_punctuations(text))\n",
    "train_punc = np.array(train_punc)\n",
    "\n",
    "val_punc = []\n",
    "for text in val_texts:\n",
    "    val_punc.append(count_punctuations(text))\n",
    "val_punc = np.array(val_punc)\n",
    "\n",
    "test_punc = []\n",
    "for text in test_texts:\n",
    "    test_punc.append(count_punctuations(text))\n",
    "test_punc = np.array(test_punc)\n",
    "print('train punc shape: ', train_punc.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06d55ee-e255-4047-b08e-e2cb194a37f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train capital shape:  (18156,)\n"
     ]
    }
   ],
   "source": [
    "def count_capital_letters(text):\n",
    "    count = sum([1 for char in text if char.isupper()])\n",
    "    return count\n",
    "\n",
    "train_capital = []\n",
    "for text in train_texts:\n",
    "    train_capital.append(count_capital_letters(text))\n",
    "train_capital = np.array(train_capital)\n",
    "\n",
    "val_capital = []\n",
    "for text in val_texts:\n",
    "    val_capital.append(count_capital_letters(text))\n",
    "val_capital = np.array(val_capital)\n",
    "\n",
    "test_capital = []\n",
    "for text in test_texts:\n",
    "    test_capital.append(count_capital_letters(text))\n",
    "test_capital = np.array(test_capital)\n",
    "print('train capital shape: ', train_capital.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab44401c-5ed6-4441-93f3-c9170c4edf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform sentiment analysis on a spanish text\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5380762c-118f-4560-a620-b78aaf0bc5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 18156/18156 [13:43<00:00, 22.05it/s]\n",
      "100%|███████████████████████████████████████| 2018/2018 [01:31<00:00, 22.01it/s]\n",
      "100%|███████████████████████████████████████| 2242/2242 [01:37<00:00, 22.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentiment shape:  (18156,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    sentiment = sentiment_analysis(text)[0]['label']\n",
    "    #remove stars\n",
    "    #if its 1 star return 1\n",
    "    if sentiment == '1 star':\n",
    "        sentiment = 1\n",
    "    #if its 2 stars return 2\n",
    "    elif sentiment == '2 stars':\n",
    "        sentiment = 2\n",
    "    #if its 3 stars return 3\n",
    "    elif sentiment == '3 stars':\n",
    "        sentiment = 3\n",
    "    #if its 4 stars return 4\n",
    "    elif sentiment == '4 stars':\n",
    "        sentiment = 4\n",
    "    #if its 5 stars return 5\n",
    "    elif sentiment == '5 stars':\n",
    "        sentiment = 5\n",
    "    return sentiment #dim: (num_samples, 1) range: [1,5]\n",
    "\n",
    "train_sentiment = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_sentiment.append(get_sentiment(text))\n",
    "train_sentiment = np.array(train_sentiment)\n",
    "\n",
    "val_sentiment = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_sentiment.append(get_sentiment(text))\n",
    "val_sentiment = np.array(val_sentiment)\n",
    "\n",
    "test_sentiment = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_sentiment.append(get_sentiment(text))\n",
    "test_sentiment = np.array(test_sentiment)\n",
    "print('train sentiment shape: ', train_sentiment.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a2ac049-cc43-4c2a-860d-3c09d742953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 18156/18156 [03:19<00:00, 91.14it/s]\n",
      "100%|███████████████████████████████████████| 2018/2018 [00:21<00:00, 91.90it/s]\n",
      "100%|███████████████████████████████████████| 2242/2242 [00:23<00:00, 93.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pos shape:  (18156, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#function to get pos tags for each category\n",
    "def get_pos(text):\n",
    "    doc = nlp(text)\n",
    "    adj_count = 0\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adp_count = 0\n",
    "    det_count = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            adj_count += 1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun_count += 1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb_count += 1\n",
    "        elif token.pos_ == 'ADP':\n",
    "            adp_count += 1\n",
    "        elif token.pos_ == 'DET':\n",
    "            det_count += 1\n",
    "    return [adj_count, noun_count, verb_count, adp_count, det_count] #dim: (num_samples, 5) \n",
    "\n",
    "train_pos = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_pos.append(get_pos(text))\n",
    "train_pos = np.array(train_pos)\n",
    "\n",
    "val_pos = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_pos.append(get_pos(text))\n",
    "val_pos = np.array(val_pos)\n",
    "\n",
    "test_pos = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_pos.append(get_pos(text))\n",
    "test_pos = np.array(test_pos)\n",
    "print('train pos shape: ', train_pos.shape) #shape: (num_samples, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b74ff8-322c-4800-a2b1-960d2c618740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "ner_analysis = pipeline(\"ner\", model=\"balamurugan1603/bert-finetuned-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d1fe338-ffd1-438f-974e-853411dc875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 18156/18156 [14:02<00:00, 21.56it/s]\n",
      "100%|███████████████████████████████████████| 2018/2018 [01:32<00:00, 21.82it/s]\n",
      "100%|███████████████████████████████████████| 2242/2242 [01:47<00:00, 20.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ner shape:  (18156, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner = ner_analysis(text)\n",
    "    loc_count = 0\n",
    "    org_count = 0\n",
    "    per_count = 0\n",
    "    misc_count = 0\n",
    "    for item in ner:\n",
    "        if item['entity'] == 'B-LOC' or item['entity'] == 'I-LOC':\n",
    "            loc_count += 1\n",
    "        elif item['entity'] == 'B-ORG' or item['entity'] == 'I-ORG':\n",
    "            org_count += 1\n",
    "        elif item['entity'] == 'B-PER' or item['entity'] == 'I-PER':\n",
    "            per_count += 1\n",
    "        elif item['entity'] == 'B-MISC' or item['entity'] == 'I-MISC':\n",
    "            misc_count += 1\n",
    "    return [loc_count, org_count, per_count, misc_count] #dim: (num_samples, 4)\n",
    "\n",
    "train_ner = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_ner.append(get_ner(text))\n",
    "train_ner = np.array(train_ner)\n",
    "\n",
    "val_ner = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_ner.append(get_ner(text))\n",
    "val_ner = np.array(val_ner)\n",
    "\n",
    "test_ner = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_ner.append(get_ner(text))\n",
    "test_ner = np.array(test_ner)\n",
    "print('train ner shape: ', train_ner.shape) #shape: (num_samples, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c68d2aa-dac0-4eec-8ec8-43aacd347712",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_punc = train_punc.reshape(-1,1)\n",
    "val_punc = val_punc.reshape(-1,1)\n",
    "test_punc = test_punc.reshape(-1,1)\n",
    "\n",
    "train_capital = train_capital.reshape(-1,1)\n",
    "val_capital = val_capital.reshape(-1,1)\n",
    "test_capital = test_capital.reshape(-1,1)\n",
    "\n",
    "train_sentiment = train_sentiment.reshape(-1,1)\n",
    "val_sentiment = val_sentiment.reshape(-1,1)\n",
    "test_sentiment = test_sentiment.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d432e9b1-9d1c-4330-a260-c534b20357f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features shape:  (18156, 12)\n",
      "[5 2 1 1 1 3 1 0 0 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "#concatenate all features\n",
    "train_features = np.concatenate((train_punc, train_capital,train_sentiment, train_pos, train_ner), axis=1) #dim: (num_samples, 11)\n",
    "val_features = np.concatenate((val_punc, val_capital,val_sentiment, val_pos, val_ner), axis=1) #dim: (num_samples, 11)\n",
    "test_features = np.concatenate((test_punc, test_capital,test_sentiment, test_pos, test_ner), axis=1) #dim: (num_samples, 11)\n",
    "print('train features shape: ', train_features.shape) #shape: (num_samples, 11)\n",
    "print(train_features[0])\n",
    "#concatenate all features\n",
    "# train_features = np.concatenate((train_pos, train_ner), axis=1) #dim: (num_samples, 11)\n",
    "# val_features = np.concatenate((val_pos, val_ner), axis=1) #dim: (num_samples, 11)\n",
    "# test_features = np.concatenate((test_pos, test_ner), axis=1) #dim: (num_samples, 11)\n",
    "# print('train features shape: ', train_features.shape) #shape: (num_samples, 11)\n",
    "# print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c55bdc50-85cb-4bfd-b151-0df3602d29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save these features in a file\n",
    "np.save('tttrain_features.npy', train_features)\n",
    "np.save('ttval_features.npy', val_features)\n",
    "np.save('tttest_features.npy', test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcf71b-28cf-4227-8593-bc0946540a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60687fc5-48dd-4f13-a45f-5232dbf168e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# # Set the number of components you want to keep\n",
    "# n_components = 15\n",
    "# # Fit PCA on the validation embeddings and transform them\n",
    "# pca = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f722eac-0bc5-481f-aa16-be080b667bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings_pca = pca.fit_transform(train_embeddings)\n",
    "# print('train embeddings pca shape: ', train_embeddings_pca.shape) #shape: (num_samples, n_components)\n",
    "\n",
    "# val_embeddings_pca = pca.transform(val_embeddings)\n",
    "# print('validation embeddings pca shape: ', val_embeddings_pca.shape) #shape: (num_samples, n_components)\n",
    "\n",
    "# test_embeddings_pca = pca.transform(test_embeddings)\n",
    "# print('test embeddings pca shape: ', test_embeddings_pca.shape) #shape: (num_samples, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47bab456-c4e9-4024-9269-0997bbc96440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pysentimiento import create_analyzer\n",
    "# analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "# text = \"Este es un ejemplo de texto con sentimiento.\"\n",
    "\n",
    "# result = analyzer.predict(text)\n",
    "\n",
    "# pos_prob = result.prob_pos\n",
    "# neg_prob = result.prob_neg\n",
    "# neu_prob = result.prob_neu\n",
    "\n",
    "# print(\"Positive Probability:\", pos_prob)\n",
    "# print(\"Negative Probability:\", neg_prob)\n",
    "# print(\"Neutral Probability:\", neu_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4c8fe8d-c6d0-4c9a-aa85-04ef212b95d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.6859e-01, -1.1747e-01,  3.3603e-01,  ...,  1.5864e-01,\n",
      "          5.5499e-01, -5.6425e-02],\n",
      "        [-2.2435e-01, -1.3418e-01, -1.6958e-01,  ...,  6.9146e-02,\n",
      "         -4.0813e-01,  4.0268e-01],\n",
      "        [ 2.9504e-01,  7.7515e-02,  1.9121e-01,  ..., -4.5071e-01,\n",
      "          2.7309e-01,  4.4976e-01],\n",
      "        ...,\n",
      "        [-2.9577e-01,  6.1761e-01,  3.8739e-01,  ..., -2.4538e-01,\n",
      "         -1.2725e-01,  8.7624e-01],\n",
      "        [-2.8540e-01, -2.3606e-01, -6.3559e-01,  ...,  2.4709e-01,\n",
      "          1.7403e-01,  6.4154e-01],\n",
      "        [ 4.9386e-02, -1.9902e-02, -6.1444e-04,  ..., -4.6091e-01,\n",
      "          5.0303e-01,  3.9415e-01]])\n",
      "tensor([1, 1, 3, 3, 5, 3, 2, 3, 4, 3, 1, 1, 3, 1, 3, 5, 5, 3, 1, 3, 5, 0, 0, 1,\n",
      "        0, 2, 1, 5, 0, 3, 1, 1])\n",
      "tensor([[ 6, 11,  5,  7,  9, 10,  6,  9,  0,  1,  0,  3],\n",
      "        [ 9, 11,  4, 10, 17, 12, 12, 13,  0,  3,  0,  0],\n",
      "        [ 6,  2,  5,  0,  4,  1,  1,  1,  0,  0,  0,  1],\n",
      "        [ 7,  7,  5,  8, 15,  5,  4, 11,  0,  0,  0,  0],\n",
      "        [ 6,  4,  3,  7, 10, 12,  5,  9,  0,  0,  0,  0],\n",
      "        [ 9, 11,  1,  5, 16, 15,  9,  9,  0,  1,  0,  3],\n",
      "        [ 2,  5,  5,  3,  1,  2,  0,  1,  0,  0,  3,  0],\n",
      "        [ 1,  1,  4,  2,  3,  7,  2,  2,  0,  0,  0,  0],\n",
      "        [ 8, 13,  2,  1,  8, 10,  5,  6,  6,  0,  2,  0],\n",
      "        [ 3,  2,  4,  1,  3,  3,  2,  2,  0,  0,  0,  0],\n",
      "        [ 6, 11,  3,  2,  6, 14,  6,  4,  0,  0,  0,  1],\n",
      "        [11,  9,  1,  0,  2,  2,  2,  1,  2,  0,  7,  0],\n",
      "        [ 4,  9,  3,  4, 10, 13,  8,  5,  0,  5,  2,  0],\n",
      "        [ 4,  1,  1,  0,  1,  6,  1,  2,  0,  0,  0,  0],\n",
      "        [ 3,  1,  1,  0,  4,  4,  4,  0,  0,  0,  0,  0],\n",
      "        [ 7,  7,  4,  2, 22, 10, 12,  9,  0,  0,  0,  0],\n",
      "        [ 6,  3,  4,  2, 14, 11,  3, 11,  0,  0,  0,  0],\n",
      "        [32, 20,  5,  1, 10,  3,  3,  2,  0,  0,  0,  0],\n",
      "        [ 4,  2,  1,  2,  6,  2,  1,  2,  0,  0,  0,  0],\n",
      "        [10, 35,  4,  1, 16, 11,  8, 10,  0,  0,  0, 13],\n",
      "        [ 8, 32,  3,  4,  9,  5,  8,  6,  0,  5, 15, 26],\n",
      "        [15,  8,  3,  7, 17,  9,  9,  8,  0,  0,  0,  0],\n",
      "        [12, 17,  4,  6, 14, 11, 10,  8,  0,  1,  0,  2],\n",
      "        [ 4,  2,  3,  2,  8,  1,  6,  1,  0,  0,  0,  0],\n",
      "        [ 6,  6,  2,  4, 13,  4, 11,  7,  1,  3,  0,  0],\n",
      "        [ 4,  7,  3,  2, 24, 18, 12, 10,  0,  3,  0,  0],\n",
      "        [ 4,  4,  3,  6,  7,  6,  4,  2,  0,  0,  0,  0],\n",
      "        [ 8, 11,  4, 10, 17,  9, 13,  9,  5,  0,  0,  2],\n",
      "        [ 5,  5,  5, 11, 16, 15,  8,  7,  0,  0,  0,  0],\n",
      "        [10, 18,  4,  5, 13,  2, 17, 17,  0,  0,  0,  0],\n",
      "        [ 6, 12,  4,  6, 16,  9, 10,  8,  0,  8,  0,  2],\n",
      "        [ 2,  0,  5,  1,  3,  3,  1,  2,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_embeddings), torch.tensor(train_labels), torch.tensor(train_features))\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(val_embeddings), torch.tensor(val_labels), torch.tensor(val_features))\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_embeddings), torch.tensor(test_labels), torch.tensor(test_features))\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for embeddings, labels, f in train_loader:\n",
    "    print(embeddings)\n",
    "    print(labels)\n",
    "    print(f)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a656e10f-25e2-4afa-9299-96ef28cce2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define neural network architecture\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# #create a neural network to use the embeddings and do classification\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "#         self.dropout1 = nn.Dropout(0.1)\n",
    "#         self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # out = F.relu(self.bn1(self.fc1(x)))\n",
    "#         out = F.relu(self.fc1(x))\n",
    "#         out = self.dropout1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "    \n",
    "# # Hyperparameters\n",
    "# input_size = 768\n",
    "# hidden_size = 128\n",
    "# num_classes = 2\n",
    "# num_epochs = 20\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77a3db57-1027-4a3b-91e0-1b8fb625f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#create a neural network to use the embeddings and do classification\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1) \n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2+12, num_classes) #11 is the number of features\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, f):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = torch.cat((out, f), dim=1)    \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "# Hyperparameters\n",
    "input_size = 768\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 38\n",
    "num_classes = 6\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16088ff5-22ca-4c06-8adc-dc296bca8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model from the neural network\n",
    "# model = Net(input_size, hidden_size, num_classes).to(device)\n",
    "model = Net(input_size, hidden_size1, hidden_size2, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9434cde1-daaf-495b-9bcc-436b37d8ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# best_val_acc = 0.0\n",
    "# total_step = len(train_loader)\n",
    "# half_epoch_step = total_step // 2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, (embeddings, labels) in tqdm(enumerate(train_loader), total=total_step, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "#         # Move tensors to the configured device\n",
    "#         embeddings = embeddings.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(embeddings)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         # Print loss every half epoch\n",
    "#         if (i+1) % half_epoch_step == 0:\n",
    "#             avg_loss = running_loss / half_epoch_step\n",
    "#             print(f\"Epoch {epoch+1}/{num_epochs} Loss after {i+1} batches: {avg_loss:.4f}\")\n",
    "#             running_loss = 0.0\n",
    "            \n",
    "#     # Validate the model\n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for embeddings, labels in val_loader:\n",
    "#             embeddings = embeddings.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(embeddings)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         # Print validation stats\n",
    "#         val_acc = 100 * correct / total\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_acc:.2f} %')\n",
    "\n",
    "#         # Save the model if the validation accuracy is better than the previous best\n",
    "#         if val_acc > best_val_acc:\n",
    "#             best_val_acc = val_acc\n",
    "#             torch.save(model.state_dict(), 'best_model.pt')\n",
    "#             print(f'Saved model with validation accuracy: {best_val_acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2692e33c-1f22-4548-8ef1-e16e1411ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  62%|███████████████▌         | 353/568 [00:00<00:00, 389.77batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss after 284 batches: 1.7755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|█████████████████████████| 568/568 [00:01<00:00, 387.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss after 568 batches: 1.4673\n",
      "Epoch 1/20 Validation Accuracy: 39.54 %, Validation Loss: 1.4296\n",
      "Saved model with validation loss: 1.4296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  62%|███████████████▌         | 354/568 [00:00<00:00, 388.82batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Loss after 284 batches: 1.3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|█████████████████████████| 568/568 [00:01<00:00, 388.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Loss after 568 batches: 1.3590\n",
      "Epoch 2/20 Validation Accuracy: 40.29 %, Validation Loss: 1.3596\n",
      "Saved model with validation loss: 1.3596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:  63%|███████████████▊         | 359/568 [00:00<00:00, 389.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss after 284 batches: 1.3112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|█████████████████████████| 568/568 [00:01<00:00, 387.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss after 568 batches: 1.2985\n",
      "Epoch 3/20 Validation Accuracy: 41.82 %, Validation Loss: 1.3121\n",
      "Saved model with validation loss: 1.3121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:  62%|███████████████▍         | 352/568 [00:00<00:00, 355.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Loss after 284 batches: 1.2582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|█████████████████████████| 568/568 [00:01<00:00, 373.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Loss after 568 batches: 1.2622\n",
      "Epoch 4/20 Validation Accuracy: 43.90 %, Validation Loss: 1.2813\n",
      "Saved model with validation loss: 1.2813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:  63%|███████████████▊         | 358/568 [00:00<00:00, 385.33batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Loss after 284 batches: 1.2211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|█████████████████████████| 568/568 [00:01<00:00, 387.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Loss after 568 batches: 1.2341\n",
      "Epoch 5/20 Validation Accuracy: 42.32 %, Validation Loss: 1.2920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:  59%|██████████████▊          | 337/568 [00:00<00:00, 345.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Loss after 284 batches: 1.1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|█████████████████████████| 568/568 [00:01<00:00, 349.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Loss after 568 batches: 1.2111\n",
      "Epoch 6/20 Validation Accuracy: 42.77 %, Validation Loss: 1.2676\n",
      "Saved model with validation loss: 1.2676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:  57%|██████████████▏          | 323/568 [00:00<00:00, 358.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Loss after 284 batches: 1.1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|█████████████████████████| 568/568 [00:01<00:00, 355.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Loss after 568 batches: 1.1927\n",
      "Epoch 7/20 Validation Accuracy: 43.36 %, Validation Loss: 1.2736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:  59%|██████████████▋          | 335/568 [00:00<00:00, 364.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Loss after 284 batches: 1.1398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|█████████████████████████| 568/568 [00:01<00:00, 364.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Loss after 568 batches: 1.1561\n",
      "Epoch 8/20 Validation Accuracy: 43.90 %, Validation Loss: 1.2599\n",
      "Saved model with validation loss: 1.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:  59%|██████████████▋          | 333/568 [00:00<00:00, 365.77batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Loss after 284 batches: 1.1246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|█████████████████████████| 568/568 [00:01<00:00, 363.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Loss after 568 batches: 1.1256\n",
      "Epoch 9/20 Validation Accuracy: 45.39 %, Validation Loss: 1.3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:  59%|██████████████▏         | 335/568 [00:00<00:00, 362.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Loss after 284 batches: 1.0777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|████████████████████████| 568/568 [00:01<00:00, 362.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Loss after 568 batches: 1.1227\n",
      "Epoch 10/20 Validation Accuracy: 45.29 %, Validation Loss: 1.3060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:  59%|██████████████▏         | 335/568 [00:00<00:00, 363.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Loss after 284 batches: 1.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|████████████████████████| 568/568 [00:01<00:00, 364.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Loss after 568 batches: 1.0933\n",
      "Epoch 11/20 Validation Accuracy: 43.95 %, Validation Loss: 1.3344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:  63%|███████████████▏        | 360/568 [00:00<00:00, 391.46batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Loss after 284 batches: 1.0505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|████████████████████████| 568/568 [00:01<00:00, 391.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Loss after 568 batches: 1.0510\n",
      "Epoch 12/20 Validation Accuracy: 43.66 %, Validation Loss: 1.3225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:  63%|███████████████▏        | 360/568 [00:00<00:00, 393.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Loss after 284 batches: 1.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|████████████████████████| 568/568 [00:01<00:00, 391.87batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Loss after 568 batches: 1.0252\n",
      "Epoch 13/20 Validation Accuracy: 43.81 %, Validation Loss: 1.3150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:  63%|███████████████▏        | 359/568 [00:00<00:00, 392.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Loss after 284 batches: 0.9820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|████████████████████████| 568/568 [00:01<00:00, 391.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Loss after 568 batches: 1.0026\n",
      "Epoch 14/20 Validation Accuracy: 44.35 %, Validation Loss: 1.3713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:  63%|███████████████▏        | 360/568 [00:00<00:00, 390.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Loss after 284 batches: 0.9643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|████████████████████████| 568/568 [00:01<00:00, 390.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Loss after 568 batches: 0.9781\n",
      "Epoch 15/20 Validation Accuracy: 44.70 %, Validation Loss: 1.3719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:  63%|███████████████▏        | 359/568 [00:00<00:00, 391.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Loss after 284 batches: 0.9348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|████████████████████████| 568/568 [00:01<00:00, 390.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Loss after 568 batches: 0.9657\n",
      "Epoch 16/20 Validation Accuracy: 44.20 %, Validation Loss: 1.4072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  63%|███████████████▏        | 360/568 [00:00<00:00, 392.58batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Loss after 284 batches: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|████████████████████████| 568/568 [00:01<00:00, 391.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Loss after 568 batches: 0.9460\n",
      "Epoch 17/20 Validation Accuracy: 43.21 %, Validation Loss: 1.4341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  63%|███████████████▏        | 360/568 [00:00<00:00, 392.65batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Loss after 284 batches: 0.8794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|████████████████████████| 568/568 [00:01<00:00, 391.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Loss after 568 batches: 0.8961\n",
      "Epoch 18/20 Validation Accuracy: 44.60 %, Validation Loss: 1.4473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:  63%|███████████████▏        | 358/568 [00:00<00:00, 391.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Loss after 284 batches: 0.8540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|████████████████████████| 568/568 [00:01<00:00, 389.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Loss after 568 batches: 0.8768\n",
      "Epoch 19/20 Validation Accuracy: 43.90 %, Validation Loss: 1.4547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:  63%|███████████████▏        | 359/568 [00:00<00:00, 392.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Loss after 284 batches: 0.8180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|████████████████████████| 568/568 [00:01<00:00, 390.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Loss after 568 batches: 0.8616\n",
      "Epoch 20/20 Validation Accuracy: 43.81 %, Validation Loss: 1.5153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_val_loss = float('inf') # initialize best validation loss to infinity\n",
    "total_step = len(train_loader)\n",
    "half_epoch_step = total_step // 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (embeddings, labels, f) in tqdm(enumerate(train_loader), total=total_step, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        # Move tensors to the configured device\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        f = f.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeddings, f)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print loss every half epoch\n",
    "        if (i+1) % half_epoch_step == 0:\n",
    "            avg_loss = running_loss / half_epoch_step\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} Loss after {i+1} batches: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # Validate the model\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for embeddings, labels, f in val_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "            f = f.to(device)\n",
    "            outputs = model(embeddings, f)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print validation stats\n",
    "        val_acc = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_acc:.2f} %, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save the model if the validation loss is better than the previous best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'Saved model with validation loss: {best_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d92a668c-3515-4613-a91c-34f71d9a39dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 71/71 [00:00<00:00, 1281.03it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    for embeddings, labels, f in tqdm(test_loader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        f = f.to(device)\n",
    "        outputs = model(embeddings,f)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    #generate classification report\n",
    "    test_report = classification_report(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0baedfa6-5d2a-4b81-8a4f-0ff6003a48d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.50      0.52       359\n",
      "           1       0.32      0.37      0.34       364\n",
      "           2       0.29      0.22      0.25       353\n",
      "           3       0.36      0.36      0.36       386\n",
      "           4       0.35      0.44      0.39       365\n",
      "           5       0.67      0.62      0.64       415\n",
      "\n",
      "    accuracy                           0.42      2242\n",
      "   macro avg       0.42      0.42      0.42      2242\n",
      "weighted avg       0.43      0.42      0.42      2242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc0a77-d4f4-49d9-b9b7-e649f9ec43c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "pyt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
