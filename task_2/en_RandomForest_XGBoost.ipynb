{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4b89ae68-16bd-495b-977a-1384389ca926",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b89ae68-16bd-495b-977a-1384389ca926",
        "outputId": "6c46f3fa-92f1-482e-938e-1f7ac7bcf0ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjid0d8ofpp4",
        "outputId": "2b259e3d-7329-4005-9170-2892381bcf35"
      },
      "id": "Kjid0d8ofpp4",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9e0db76b-a714-46fe-8546-4ca9ddf12e64",
      "metadata": {
        "id": "9e0db76b-a714-46fe-8546-4ca9ddf12e64"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2f3893dc-528f-4fc1-840d-e4ff1623bb46",
      "metadata": {
        "id": "2f3893dc-528f-4fc1-840d-e4ff1623bb46"
      },
      "outputs": [],
      "source": [
        "seed = 25\n",
        "# random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3092c446-2756-4864-9761-9a2a61fce91f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3092c446-2756-4864-9761-9a2a61fce91f",
        "outputId": "175cba21-dbfa-4b71-db60-fb6e2349a868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                               text label\n",
            "0   6239  It was not until many years later that it coul...     A\n",
            "1   9255  Users can then pin these images to their profi...     F\n",
            "2   1674  The best songs are those that I can sing along...     B\n",
            "3   5001  I found this book to be poorly written. It was...     D\n",
            "4  20779  Regulates the application of the EU tariff quo...     E\n"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/datasets/subtask_2/en/train.tsv',sep='\\t')\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8f0c27a0-ce1b-4058-a542-dccfcd447764",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f0c27a0-ce1b-4058-a542-dccfcd447764",
        "outputId": "24dabfb8-f0bf-4714-9d99-879c8b0749c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels:  {'B', 'C', 'A', 'E', 'F', 'D'}\n"
          ]
        }
      ],
      "source": [
        "train_data_texts = train_data['text'].to_list()\n",
        "train_data_labels = train_data['label'].to_list()\n",
        "print(\"Unique labels: \", set(train_data_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d39aa787-11cc-47af-8814-595691a510f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39aa787-11cc-47af-8814-595691a510f0",
        "outputId": "cc0a5f58-8ad2-438d-dada-253424722036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data size:  18156\n",
            "validation data size:  2018\n",
            "test data size:  2242\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data_texts = train_data['text'].to_list()\n",
        "train_data_labels = train_data['label'].to_list()\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(train_data_texts, train_data_labels, test_size=0.1, random_state=25)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=25)\n",
        "print('train data size: ', len(train_texts))\n",
        "print('validation data size: ', len(val_texts))\n",
        "print('test data size: ', len(test_texts))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n51tLechzt7",
        "outputId": "d5346d26-8aba-4fc0-a234-c9177fec919c"
      },
      "id": "5n51tLechzt7",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Convert train and test texts to BERT embeddings\n",
        "train_embeddings = []\n",
        "for text in tqdm(train_texts):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    train_embeddings.append(outputs.last_hidden_state[:,0,:].cpu().numpy())\n",
        "\n",
        "test_embeddings = []\n",
        "for text in tqdm(test_texts):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    test_embeddings.append(outputs.last_hidden_state[:,0,:].cpu().numpy())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wTUtVcehaR4",
        "outputId": "f9546135-b615-4771-caa8-bfced1bb009f"
      },
      "id": "0wTUtVcehaR4",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 18156/18156 [04:49<00:00, 62.62it/s]\n",
            "100%|██████████| 2242/2242 [00:32<00:00, 68.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_embeddings = [emb.reshape(-1) for emb in train_embeddings]\n",
        "# test_embeddings = [emb.reshape(-1) for emb in test_embeddings]\n",
        "# Train Random Forest classifier on train embeddings\n",
        "rf_classifier = RandomForestClassifier()\n",
        "rf_classifier.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Predict labels for test embeddings\n",
        "pred_labels = rf_classifier.predict(test_embeddings)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(test_labels, pred_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y9B82aEqJGb",
        "outputId": "49ec910c-96bc-4bdd-a7e9-0d343ab22ae7"
      },
      "id": "_y9B82aEqJGb",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.35      0.43      0.39       359\n",
            "           B       0.22      0.20      0.21       364\n",
            "           C       0.25      0.19      0.22       353\n",
            "           D       0.31      0.34      0.33       386\n",
            "           E       0.30      0.28      0.29       365\n",
            "           F       0.53      0.55      0.54       415\n",
            "\n",
            "    accuracy                           0.34      2242\n",
            "   macro avg       0.33      0.33      0.33      2242\n",
            "weighted avg       0.33      0.34      0.33      2242\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  XGBoost is a gradient boosting algorithm, which means that it builds a series of weak learners sequentially, \n",
        "  where each new learner tries to improve the errors of the previous ones. \n",
        "  On the other hand, Random Forest is a bagging algorithm that builds multiple decision trees \n",
        "  in parallel and combines their predictions by taking the majority vote.\n",
        "\n",
        "\n",
        "  XGBoost is a powerful algorithm that can handle complex relationships between features and \n",
        "  the target variable, especially for large datasets, while Random Forest is a reliable \n",
        "  algorithm that is easier to interpret and generally works well for smaller datasets with fewer features. \n",
        "'''\n",
        "import xgboost as xgb\n",
        "import re\n",
        "# training xgboost \n",
        "\n",
        "# XGBoost only supports ASCII characters, so you may need to preprocess your data to remove any non-ASCII characters before feeding it to XGBoost.\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Replace any non-ASCII characters with their ASCII equivalents\n",
        "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
        "    # Remove any remaining non-alphanumeric characters and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
        "    return text\n",
        "\n",
        "# Preprocess the train and test texts\n",
        "train_texts_ascii = [preprocess_text(text) for text in train_texts]\n",
        "test_texts_ascii = [preprocess_text(text) for text in test_texts]\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Convert train and test texts to BERT embeddings\n",
        "train_embeddings = []\n",
        "for text in tqdm(train_texts_ascii):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    train_embeddings.append(outputs.last_hidden_state[:,0,:].cpu().numpy())\n",
        "\n",
        "test_embeddings = []\n",
        "for text in tqdm(test_texts_ascii):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    test_embeddings.append(outputs.last_hidden_state[:,0,:].cpu().numpy())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYwDg4e2iZel",
        "outputId": "cd72a0de-bb9e-403e-f68a-6261a942f316"
      },
      "id": "DYwDg4e2iZel",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 18156/18156 [04:28<00:00, 67.67it/s]\n",
            "100%|██████████| 2242/2242 [00:32<00:00, 68.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert labels to integer labels\n",
        "le = LabelEncoder()\n",
        "train_labels_encoded = le.fit_transform(train_labels)\n",
        "test_labels_encoded = le.transform(test_labels)\n",
        "\n",
        "# Train XGBoost classifier on train embeddings\n",
        "params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': 6,\n",
        "    'tree_method': 'gpu_hist'\n",
        "}\n",
        "\n",
        "# create the XGBoost classifier\n",
        "xgb_classifier = xgb.XGBClassifier(**params)\n",
        "train_embeddings = np.vstack(train_embeddings)\n",
        "xgb_classifier.fit(train_embeddings, train_labels_encoded)\n",
        "\n",
        "print(\"embed size check\", np.array(test_embeddings).shape)\n",
        "\n",
        "# Predict labels for test embeddings\n",
        "test_embeddings = np.vstack(test_embeddings)\n",
        "pred_labels_encoded = xgb_classifier.predict(np.array(test_embeddings))\n",
        "\n",
        "# Convert predicted integer labels back to original labels\n",
        "pred_labels = le.inverse_transform(pred_labels_encoded)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(test_labels, pred_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPFNoKojup3w",
        "outputId": "c2b69bef-1a9b-4efc-ec7d-56ce0f40678b"
      },
      "id": "wPFNoKojup3w",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed size check (2242, 1, 768)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.40      0.41      0.40       359\n",
            "           B       0.23      0.23      0.23       364\n",
            "           C       0.24      0.24      0.24       353\n",
            "           D       0.31      0.31      0.31       386\n",
            "           E       0.27      0.29      0.28       365\n",
            "           F       0.61      0.59      0.60       415\n",
            "\n",
            "    accuracy                           0.35      2242\n",
            "   macro avg       0.34      0.34      0.34      2242\n",
            "weighted avg       0.35      0.35      0.35      2242\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grid search to tune the parameters of xgboost\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "xgb_classifier = XGBClassifier(tree_method='gpu_hist', n_jobs=-1)\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "params = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "}\n",
        "\n",
        "# Define the grid search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_classifier, \n",
        "    param_grid=params, \n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "# Train the grid search\n",
        "grid_search.fit(train_embeddings, train_labels_encoded)\n",
        "\n",
        "# Print the best hyperparameters and score\n",
        "print('Best parameters:', grid_search.best_params_)\n",
        "print('Best score:', grid_search.best_score_)\n",
        "\n",
        "# Use the best model to predict the test set and evaluate the performance\n",
        "best_xgb = grid_search.best_estimator_\n",
        "test_pred = best_xgb.predict(test_embeddings)\n",
        "test_acc = accuracy_score(test_labels_encoded, test_pred)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yva1vFKL1mCn",
        "outputId": "ffb05e3b-a4f1-4f59-b38a-9de6f59d4ebc"
      },
      "id": "yva1vFKL1mCn",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
            "Best score: 0.355970478078872\n",
            "Test accuracy: 0.35459411239964317\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyt",
      "language": "python",
      "name": "pyt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}